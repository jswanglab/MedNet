{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誘導式深度學習 (Highlighted DL) 效果在 TensorFlow + Keras 上的實證"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實驗背景\n",
    "　　這是基於我的碩士論文「以誘導式深度學習為基礎之配藥核實技術及應用」所延伸的實驗（多國發明專利申請中，論文未公開）。在原始論文裡，我採用了 YOLO v2、ResNet 與 SE-ResNet 三種網路，證明「誘導式深度學習」能有效解決**訓練樣本稀少**、**藥物外觀相似**與**藥物種類繁多**等藥物識別上的難題，不但將藥物辨識率從 86% 提升至 99.8%，更採用 NVIDIA® Jetson TX2 嵌入式系統與模型結構優化技術，實現 6.23 幀 / 秒的實時藥物識別產品雛型。\n",
    "\n",
    "　　由於 YOLO v2、ResNet 與 SE-ResNet 皆屬較龐大的網路，所以，為了證明「誘導式深度學習」在簡單的 CNN 網路上**也能有卓越的識別率優化效果**，我將使用論文中未出現的 TensorFlow 與 Keras 框架重新進行實驗，也就是接下來這份文件中的內容。\n",
    "\n",
    "\n",
    "### 數據說明\n",
    "　　這項實驗所採用的數據與原始論文相同：成人錠劑**排藥包裝 (Blister Package)** 共 250 類，每個類別正、反面各 72 張，總計 36,000 張影像。對了！這是一份公開數據集，您可以在 [Google Drive](https://drive.google.com/folderview?id=1cV7JVYGRxcm9DY0o7BMJHTCRdo_RW17n) 或 [Baidu Pan](https://pan.baidu.com/s/1amfARVIhGIfYVIIfB9qFRg) 下載，供學術研究與非商業性質的實驗使用。\n",
    "\n",
    "\n",
    "### 實驗說明\n",
    "　　與原始論文相同，我將使用排藥的 **1. 正面原始影像**、**2. 背面原始影像**，與經過「誘導式深度學習」技術生成的 **3. 雙面拼整影像**資料集，對相同的 CNN 網路進行訓練與測試，目的在於觀察「誘導式深度學習」技術的採用與否，對排藥識別效果的影響會有多大。\n",
    "  \n",
    "　　在訓練樣本、測試樣本的分配上，將依照 3:1 的比例，對每類別 72 張圖像**隨機抽取** 54 張作為訓練集 (Training-set)、18 張作為測試集 (Testing-set)，總計訓練樣本 13,500 張，訓練樣本外 (Out-of-sample) 的測試樣本 4,500 張。訓練時，還會從每類別 54 張訓練圖像中再**隨機抽取** 18 張作為驗證集 (Validation-set)。\n",
    "  \n",
    "　　介紹完了，我們開始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 環境參數設定\n",
    "設置載入的套件與資料集路徑。在本實驗中，我將採用 150 x 150 的圖像輸入尺寸進行模型的訓練與測試，而非原始論文的 224 x 224。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image_size = 150\n",
    "\n",
    "# 正面原始影像的訓練與測試圖像路徑。\n",
    "front_train_dir = \"/home/ubuntu/Desktop/High/OriginalFrontTrain\"\n",
    "front_test_dir = \"/home/ubuntu/Desktop/High/OriginalFrontTest\"\n",
    "\n",
    "# 背面原始影像的訓練與測試圖像路徑。\n",
    "back_train_dir = \"/home/ubuntu/Desktop/High/OriginalBackTrain\"\n",
    "back_test_dir = \"/home/ubuntu/Desktop/High/OriginalBackTest\"\n",
    "\n",
    "# 雙面拼整影像的訓練與測試圖像路徑。\n",
    "high_train_dir = \"/home/ubuntu/Desktop/High/HighlightedTrain\"\n",
    "high_test_dir = \"/home/ubuntu/Desktop/High/HighlightedTest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 建立簡單的 CNN 模型（三種資料集都使用相同的模型結構）\n",
    "卷積神經網路中採用 Dropout 防止數據過擬合；最後一個全連接層使用 Softmax 作為激活函數，對 250 種排藥類別進行預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 250)               32250     \n",
      "=================================================================\n",
      "Total params: 494,298\n",
      "Trainable params: 494,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 訓練與識別正面原始影像的模型\n",
    "model_front = models.Sequential()\n",
    "model_front.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "\n",
    "model_front.add(layers.Flatten())\n",
    "model_front.add(layers.Dense(128, activation = 'relu'))\n",
    "model_front.add(Dropout(0.5))\n",
    "model_front.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "\n",
    "# 訓練與識別背面原始影像的模型\n",
    "model_back = models.Sequential()\n",
    "model_back.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "\n",
    "model_back.add(layers.Flatten())\n",
    "model_back.add(layers.Dense(128, activation = 'relu'))\n",
    "model_back.add(Dropout(0.5))\n",
    "model_back.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "\n",
    "# 訓練與識別雙面拼整影像的模型\n",
    "model_high = models.Sequential()\n",
    "model_high.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "\n",
    "model_high.add(layers.Flatten())\n",
    "model_high.add(layers.Dense(128, activation = 'relu'))\n",
    "model_high.add(Dropout(0.5))\n",
    "model_high.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "\n",
    "# 看看模型長什麼樣子\n",
    "model_high.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 編譯模型\n",
    "使用 'categorical_crossentropy' 作為損失函數，讓「網路輸出」的機率分佈與「真實標籤」的分佈盡可能一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_front.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_back.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_high.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 採用 Generator 批量生成訓練與驗證資料\n",
    "使用驗證資料集來確認該在何時執行 Early Stopping，以便稍後（Step 8）選用最佳模型進行測試資料集的效果評估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of front images:\n",
      "[Training set]\n",
      "Found 9000 images belonging to 250 classes.\n",
      "[Validation set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Status of back images:\n",
      "[Training set]\n",
      "Found 9000 images belonging to 250 classes.\n",
      "[Validation set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Status of highlighted images:\n",
      "[Training set]\n",
      "Found 9000 images belonging to 250 classes.\n",
      "[Validation set]\n",
      "Found 4500 images belonging to 250 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 從同一個 training 資料路徑下拆分出 training 與 validation 的數據集。\n",
    "# 設定 validation_split=0.35，從每類 54 張訓練樣本中抽取 18 張作為驗證樣本。\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.35)\n",
    "\n",
    "# 正面原始影像的 Generator\n",
    "\n",
    "print(\"Status of front images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "front_train_gen = data_generator.flow_from_directory(\n",
    "        front_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical', # 多類別，所以使用 categorical 標籤，返回 2D 的 one-hot 編碼標籤。\n",
    "        batch_size = 36,\n",
    "        subset = \"training\") # 因為啟用了 validation_split，故可設定 train_generator 放的是「訓練子集」。\n",
    "\n",
    "print(\"[Validation set]\")\n",
    "\n",
    "front_validation_gen = data_generator.flow_from_directory(\n",
    "        front_train_dir, # 一樣從 training 的路徑下抓取驗證用數據。\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"validation\") # 因為啟用了 validation_split，故可設定 validation_generator 放的是「驗證子集」。\n",
    "\n",
    "# 背面原始影像的 Generator\n",
    "\n",
    "print(\"\\nStatus of back images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "back_train_gen = data_generator.flow_from_directory(\n",
    "        back_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"training\")\n",
    "\n",
    "print(\"[Validation set]\")\n",
    "\n",
    "back_validation_gen = data_generator.flow_from_directory(\n",
    "        back_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"validation\")\n",
    "\n",
    "# 雙面拼整影像的 Generator\n",
    "\n",
    "print(\"\\nStatus of highlighted images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "high_train_gen = data_generator.flow_from_directory(\n",
    "        high_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"training\")\n",
    "\n",
    "print(\"[Validation set]\")\n",
    "\n",
    "high_validation_gen = data_generator.flow_from_directory(\n",
    "        high_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 定義 Callback Function 以計算 Percision / Recall / F1-Score\n",
    "Keras 提供的...只能在xxx計算一次，不是每個 Epoch。此外，Generator 若要使用 Callback 函式，需有特別的設計方法。\n",
    "\n",
    "### 參考資料\n",
    "* https://medium.com/@thongonary/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2\n",
    "* https://github.com/keras-team/keras/issues/10472#issuecomment-472543538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "# 紀錄每個 Epoch 訓練結束時，使用驗證集測試得到的 F1-score 結果\n",
    "front_f1s = []\n",
    "back_f1s = []\n",
    "high_f1s = []\n",
    "\n",
    "class MetricsFront(Callback):\n",
    "\n",
    "    def __init__(self, val_data = front_validation_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        batches = len(self.validation_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        val_pred = np.zeros((total, 250))\n",
    "        val_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xVal, yVal = next(self.validation_data)\n",
    "            val_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xVal)).round()\n",
    "            val_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yVal\n",
    "            \n",
    "        val_pred = np.squeeze(val_pred)\n",
    "        _val_f1 = f1_score(val_true, val_pred, average = 'micro')\n",
    "        _val_precision = precision_score(val_true, val_pred, average = 'micro')\n",
    "        _val_recall = recall_score(val_true, val_pred, average = 'micro')\n",
    "        \n",
    "        front_f1s.append(_val_f1)\n",
    "\n",
    "        print(\"- val_f1: %f - val_precision: %f - val_recall: %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "    \n",
    "class MetricsBack(Callback):\n",
    "\n",
    "    def __init__(self, val_data = back_validation_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        batches = len(self.validation_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        val_pred = np.zeros((total, 250))\n",
    "        val_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xVal, yVal = next(self.validation_data)\n",
    "            val_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xVal)).round()\n",
    "            val_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yVal\n",
    "            \n",
    "        val_pred = np.squeeze(val_pred)\n",
    "        _val_f1 = f1_score(val_true, val_pred, average = 'micro')\n",
    "        _val_precision = precision_score(val_true, val_pred, average = 'micro')\n",
    "        _val_recall = recall_score(val_true, val_pred, average = 'micro')\n",
    "        \n",
    "        back_f1s.append(_val_f1)\n",
    "        \n",
    "        print(\"- val_f1: %f - val_precision: %f - val_recall: %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "    \n",
    "class MetricsHigh(Callback):\n",
    "\n",
    "    def __init__(self, val_data = high_validation_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        batches = len(self.validation_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        val_pred = np.zeros((total, 250))\n",
    "        val_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xVal, yVal = next(self.validation_data)\n",
    "            val_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xVal)).round()\n",
    "            val_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yVal\n",
    "            \n",
    "        val_pred = np.squeeze(val_pred)\n",
    "        _val_f1 = f1_score(val_true, val_pred, average = 'micro')\n",
    "        _val_precision = precision_score(val_true, val_pred, average = 'micro')\n",
    "        _val_recall = recall_score(val_true, val_pred, average = 'micro')\n",
    "        \n",
    "        high_f1s.append(_val_f1)\n",
    "        \n",
    "        print(\"- val_f1: %f - val_precision: %f - val_recall: %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. 訓練與驗證深度學習網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front images training:\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 138s 552ms/step - loss: 5.4070 - acc: 0.0066 - val_loss: 5.2364 - val_acc: 0.0091\n",
      "- val_f1: 0.000000 - val_precision: 0.000000 - val_recall: 0.000000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 4.9640 - acc: 0.0217 - val_loss: 4.5970 - val_acc: 0.0587\n",
      "- val_f1: 0.000000 - val_precision: 0.000000 - val_recall: 0.000000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 4.4790 - acc: 0.0546 - val_loss: 4.2231 - val_acc: 0.0931\n",
      "- val_f1: 0.009288 - val_precision: 0.954545 - val_recall: 0.004667\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 4.0550 - acc: 0.0906 - val_loss: 3.7718 - val_acc: 0.1698\n",
      "- val_f1: 0.017613 - val_precision: 0.952381 - val_recall: 0.008889\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 122s 486ms/step - loss: 3.6821 - acc: 0.1318 - val_loss: 3.5351 - val_acc: 0.2238\n",
      "- val_f1: 0.047629 - val_precision: 0.924370 - val_recall: 0.024444\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 3.3885 - acc: 0.1756 - val_loss: 3.2094 - val_acc: 0.2371\n",
      "- val_f1: 0.103549 - val_precision: 0.855172 - val_recall: 0.055111\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 3.1618 - acc: 0.2117 - val_loss: 3.0518 - val_acc: 0.2791\n",
      "- val_f1: 0.107315 - val_precision: 0.944649 - val_recall: 0.056889\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 2.9537 - acc: 0.2510 - val_loss: 2.5925 - val_acc: 0.3658\n",
      "- val_f1: 0.185917 - val_precision: 0.908382 - val_recall: 0.103556\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 122s 486ms/step - loss: 2.7790 - acc: 0.2792 - val_loss: 2.5189 - val_acc: 0.3856\n",
      "- val_f1: 0.214961 - val_precision: 0.941379 - val_recall: 0.121333\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 2.6161 - acc: 0.3084 - val_loss: 2.7191 - val_acc: 0.3202\n",
      "- val_f1: 0.263048 - val_precision: 0.901170 - val_recall: 0.154000\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 2.4853 - acc: 0.3333 - val_loss: 2.3166 - val_acc: 0.3978\n",
      "- val_f1: 0.319711 - val_precision: 0.858252 - val_recall: 0.196444\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 2.3486 - acc: 0.3632 - val_loss: 2.2566 - val_acc: 0.3969\n",
      "- val_f1: 0.339757 - val_precision: 0.862319 - val_recall: 0.211556\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 2.2324 - acc: 0.3877 - val_loss: 2.1377 - val_acc: 0.4373\n",
      "- val_f1: 0.404510 - val_precision: 0.921739 - val_recall: 0.259111\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 121s 483ms/step - loss: 2.1274 - acc: 0.4147 - val_loss: 2.0216 - val_acc: 0.4407\n",
      "- val_f1: 0.397927 - val_precision: 0.893023 - val_recall: 0.256000\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 2.0358 - acc: 0.4359 - val_loss: 2.0063 - val_acc: 0.4413\n",
      "- val_f1: 0.418001 - val_precision: 0.865318 - val_recall: 0.275556\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.9607 - acc: 0.4544 - val_loss: 1.6772 - val_acc: 0.5422\n",
      "- val_f1: 0.476472 - val_precision: 0.908693 - val_recall: 0.322889\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.8958 - acc: 0.4704 - val_loss: 1.6703 - val_acc: 0.5280\n",
      "- val_f1: 0.507367 - val_precision: 0.908257 - val_recall: 0.352000\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.8153 - acc: 0.4926 - val_loss: 1.7082 - val_acc: 0.5198\n",
      "- val_f1: 0.469716 - val_precision: 0.853630 - val_recall: 0.324000\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.7641 - acc: 0.5056 - val_loss: 1.6512 - val_acc: 0.5424\n",
      "- val_f1: 0.483117 - val_precision: 0.896386 - val_recall: 0.330667\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 122s 486ms/step - loss: 1.6995 - acc: 0.5181 - val_loss: 1.5876 - val_acc: 0.5673\n",
      "- val_f1: 0.547833 - val_precision: 0.867952 - val_recall: 0.400222\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 122s 487ms/step - loss: 1.6609 - acc: 0.5208 - val_loss: 1.5373 - val_acc: 0.5604\n",
      "- val_f1: 0.538735 - val_precision: 0.885613 - val_recall: 0.387111\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.5989 - acc: 0.5376 - val_loss: 1.4956 - val_acc: 0.5727\n",
      "- val_f1: 0.538673 - val_precision: 0.862880 - val_recall: 0.391556\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.5625 - acc: 0.5494 - val_loss: 1.6572 - val_acc: 0.5340\n",
      "- val_f1: 0.542172 - val_precision: 0.811337 - val_recall: 0.407111\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.5360 - acc: 0.5550 - val_loss: 1.6356 - val_acc: 0.5316\n",
      "- val_f1: 0.540012 - val_precision: 0.835661 - val_recall: 0.398889\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 122s 486ms/step - loss: 1.4689 - acc: 0.5669 - val_loss: 1.5561 - val_acc: 0.5329\n",
      "- val_f1: 0.533488 - val_precision: 0.770457 - val_recall: 0.408000\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.4587 - acc: 0.5790 - val_loss: 1.7225 - val_acc: 0.5184\n",
      "- val_f1: 0.522515 - val_precision: 0.763675 - val_recall: 0.397111\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 122s 486ms/step - loss: 1.4116 - acc: 0.5873 - val_loss: 1.3704 - val_acc: 0.5927\n",
      "- val_f1: 0.581268 - val_precision: 0.826639 - val_recall: 0.448222\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.3621 - acc: 0.5970 - val_loss: 1.3543 - val_acc: 0.6120\n",
      "- val_f1: 0.565316 - val_precision: 0.877866 - val_recall: 0.416889\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.3474 - acc: 0.6022 - val_loss: 1.3722 - val_acc: 0.5973\n",
      "- val_f1: 0.580965 - val_precision: 0.844134 - val_recall: 0.442889\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.3303 - acc: 0.6163 - val_loss: 1.4087 - val_acc: 0.5833\n",
      "- val_f1: 0.581793 - val_precision: 0.797292 - val_recall: 0.458000\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.3182 - acc: 0.6130 - val_loss: 1.2713 - val_acc: 0.6260\n",
      "- val_f1: 0.600762 - val_precision: 0.822076 - val_recall: 0.473333\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.2795 - acc: 0.6181 - val_loss: 1.4072 - val_acc: 0.5953\n",
      "- val_f1: 0.593518 - val_precision: 0.805101 - val_recall: 0.470000\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 1.2582 - acc: 0.6318 - val_loss: 1.4549 - val_acc: 0.5873\n",
      "- val_f1: 0.564518 - val_precision: 0.784500 - val_recall: 0.440889\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 122s 487ms/step - loss: 1.2465 - acc: 0.6306 - val_loss: 1.3600 - val_acc: 0.6073\n",
      "- val_f1: 0.594740 - val_precision: 0.817652 - val_recall: 0.467333\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.2357 - acc: 0.6383 - val_loss: 1.1382 - val_acc: 0.6660\n",
      "- val_f1: 0.649566 - val_precision: 0.856052 - val_recall: 0.523333\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.1894 - acc: 0.6511 - val_loss: 1.5185 - val_acc: 0.5680\n",
      "- val_f1: 0.562404 - val_precision: 0.752140 - val_recall: 0.449111\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.2178 - acc: 0.6396 - val_loss: 1.3260 - val_acc: 0.6084\n",
      "- val_f1: 0.584710 - val_precision: 0.790310 - val_recall: 0.464000\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.1699 - acc: 0.6484 - val_loss: 1.4701 - val_acc: 0.5824\n",
      "- val_f1: 0.568734 - val_precision: 0.796637 - val_recall: 0.442222\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.1615 - acc: 0.6510 - val_loss: 1.3259 - val_acc: 0.6160\n",
      "- val_f1: 0.607197 - val_precision: 0.799855 - val_recall: 0.489333\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.1502 - acc: 0.6627 - val_loss: 1.2674 - val_acc: 0.6302\n",
      "- val_f1: 0.618929 - val_precision: 0.816230 - val_recall: 0.498444\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 121s 485ms/step - loss: 1.1365 - acc: 0.6641 - val_loss: 1.8218 - val_acc: 0.5120\n",
      "- val_f1: 0.510385 - val_precision: 0.677076 - val_recall: 0.409556\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.1480 - acc: 0.6631 - val_loss: 1.1511 - val_acc: 0.6718\n",
      "- val_f1: 0.653492 - val_precision: 0.835873 - val_recall: 0.536444\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 1.1115 - acc: 0.6677 - val_loss: 1.5718 - val_acc: 0.5756\n",
      "- val_f1: 0.579867 - val_precision: 0.742798 - val_recall: 0.475556\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.0668 - acc: 0.6822 - val_loss: 1.2976 - val_acc: 0.6273\n",
      "- val_f1: 0.611813 - val_precision: 0.801079 - val_recall: 0.494889\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.0833 - acc: 0.6770 - val_loss: 1.5273 - val_acc: 0.5747\n",
      "- val_f1: 0.574511 - val_precision: 0.716849 - val_recall: 0.479333\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.0851 - acc: 0.6847 - val_loss: 1.2106 - val_acc: 0.6542\n",
      "- val_f1: 0.651292 - val_precision: 0.806897 - val_recall: 0.546000\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.0651 - acc: 0.6816 - val_loss: 1.4293 - val_acc: 0.5947\n",
      "- val_f1: 0.597947 - val_precision: 0.762578 - val_recall: 0.491778\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.0676 - acc: 0.6871 - val_loss: 1.9508 - val_acc: 0.5038\n",
      "- val_f1: 0.493034 - val_precision: 0.650055 - val_recall: 0.397111\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 1.0827 - acc: 0.6873 - val_loss: 1.4425 - val_acc: 0.6011\n",
      "- val_f1: 0.610607 - val_precision: 0.748629 - val_recall: 0.515556\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.0614 - acc: 0.6847 - val_loss: 1.5107 - val_acc: 0.5831\n",
      "- val_f1: 0.563324 - val_precision: 0.778518 - val_recall: 0.441333\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 122s 486ms/step - loss: 1.0718 - acc: 0.6831 - val_loss: 1.3400 - val_acc: 0.6158\n",
      "- val_f1: 0.605382 - val_precision: 0.755822 - val_recall: 0.504889\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.0527 - acc: 0.6881 - val_loss: 1.5124 - val_acc: 0.5904\n",
      "- val_f1: 0.584569 - val_precision: 0.753949 - val_recall: 0.477333\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 121s 484ms/step - loss: 1.0424 - acc: 0.6983 - val_loss: 1.2599 - val_acc: 0.6422\n",
      "- val_f1: 0.642763 - val_precision: 0.790019 - val_recall: 0.541778\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 121s 485ms/step - loss: 1.0784 - acc: 0.6896 - val_loss: 1.3879 - val_acc: 0.5964\n",
      "- val_f1: 0.577759 - val_precision: 0.791570 - val_recall: 0.454889\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.0806 - acc: 0.6931 - val_loss: 1.2904 - val_acc: 0.6298\n",
      "- val_f1: 0.626371 - val_precision: 0.817239 - val_recall: 0.507778\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 127s 507ms/step - loss: 1.0466 - acc: 0.6998 - val_loss: 1.4001 - val_acc: 0.6047\n",
      "- val_f1: 0.598123 - val_precision: 0.789148 - val_recall: 0.481556\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.0718 - acc: 0.6924 - val_loss: 1.5019 - val_acc: 0.6213\n",
      "- val_f1: 0.626247 - val_precision: 0.737794 - val_recall: 0.544000\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.0592 - acc: 0.6974 - val_loss: 1.6863 - val_acc: 0.5742\n",
      "- val_f1: 0.590391 - val_precision: 0.697126 - val_recall: 0.512000\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.0837 - acc: 0.6942 - val_loss: 1.4769 - val_acc: 0.6133\n",
      "- val_f1: 0.596697 - val_precision: 0.744681 - val_recall: 0.497778\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 123s 494ms/step - loss: 1.0553 - acc: 0.6980 - val_loss: 1.3922 - val_acc: 0.6304\n",
      "- val_f1: 0.617818 - val_precision: 0.759067 - val_recall: 0.520889\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.0429 - acc: 0.7058 - val_loss: 1.2770 - val_acc: 0.6498\n",
      "- val_f1: 0.596238 - val_precision: 0.819914 - val_recall: 0.468444\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.0524 - acc: 0.7039 - val_loss: 2.2075 - val_acc: 0.4918\n",
      "- val_f1: 0.468923 - val_precision: 0.624077 - val_recall: 0.375556\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.0329 - acc: 0.7022 - val_loss: 1.6967 - val_acc: 0.5558\n",
      "- val_f1: 0.540798 - val_precision: 0.698698 - val_recall: 0.441111\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.0437 - acc: 0.7021 - val_loss: 1.7148 - val_acc: 0.5631\n",
      "- val_f1: 0.551015 - val_precision: 0.704498 - val_recall: 0.452444\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.0315 - acc: 0.7050 - val_loss: 1.6508 - val_acc: 0.5971\n",
      "- val_f1: 0.590984 - val_precision: 0.684411 - val_recall: 0.520000\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.0524 - acc: 0.7029 - val_loss: 1.8566 - val_acc: 0.5762\n",
      "- val_f1: 0.578378 - val_precision: 0.687156 - val_recall: 0.499333\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.0542 - acc: 0.7070 - val_loss: 1.8765 - val_acc: 0.5369\n",
      "- val_f1: 0.494728 - val_precision: 0.658419 - val_recall: 0.396222\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.0719 - acc: 0.7030 - val_loss: 1.8471 - val_acc: 0.5698\n",
      "- val_f1: 0.555193 - val_precision: 0.673534 - val_recall: 0.472222\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.0458 - acc: 0.7072 - val_loss: 1.5357 - val_acc: 0.6067\n",
      "- val_f1: 0.592543 - val_precision: 0.735265 - val_recall: 0.496222\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 123s 490ms/step - loss: 1.0650 - acc: 0.7003 - val_loss: 1.9881 - val_acc: 0.5742\n",
      "- val_f1: 0.566105 - val_precision: 0.664868 - val_recall: 0.492889\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.0906 - acc: 0.7021 - val_loss: 2.0616 - val_acc: 0.5131\n",
      "- val_f1: 0.508457 - val_precision: 0.634430 - val_recall: 0.424222\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.0635 - acc: 0.7038 - val_loss: 1.3775 - val_acc: 0.6373\n",
      "- val_f1: 0.620881 - val_precision: 0.763124 - val_recall: 0.523333\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.0733 - acc: 0.7066 - val_loss: 1.8238 - val_acc: 0.5489\n",
      "- val_f1: 0.527514 - val_precision: 0.676164 - val_recall: 0.432444\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.0933 - acc: 0.7028 - val_loss: 1.7147 - val_acc: 0.5347\n",
      "- val_f1: 0.523796 - val_precision: 0.693915 - val_recall: 0.420667\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.0817 - acc: 0.7004 - val_loss: 1.5562 - val_acc: 0.6000\n",
      "- val_f1: 0.589446 - val_precision: 0.725325 - val_recall: 0.496444\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.0950 - acc: 0.7088 - val_loss: 2.0645 - val_acc: 0.5493\n",
      "- val_f1: 0.551224 - val_precision: 0.660664 - val_recall: 0.472889\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.0977 - acc: 0.6992 - val_loss: 1.7147 - val_acc: 0.5764\n",
      "- val_f1: 0.569917 - val_precision: 0.731291 - val_recall: 0.466889\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.0859 - acc: 0.7010 - val_loss: 2.5612 - val_acc: 0.4718\n",
      "- val_f1: 0.456585 - val_precision: 0.562053 - val_recall: 0.384444\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.1187 - acc: 0.6977 - val_loss: 1.9746 - val_acc: 0.4956\n",
      "- val_f1: 0.488516 - val_precision: 0.723822 - val_recall: 0.368667\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.1205 - acc: 0.6993 - val_loss: 2.0716 - val_acc: 0.4822\n",
      "- val_f1: 0.452072 - val_precision: 0.607424 - val_recall: 0.360000\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 122s 488ms/step - loss: 1.0932 - acc: 0.7023 - val_loss: 1.6022 - val_acc: 0.6193\n",
      "- val_f1: 0.611929 - val_precision: 0.730129 - val_recall: 0.526667\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.0928 - acc: 0.6989 - val_loss: 1.9424 - val_acc: 0.5198\n",
      "- val_f1: 0.482827 - val_precision: 0.658602 - val_recall: 0.381111\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.1436 - acc: 0.6894 - val_loss: 2.1243 - val_acc: 0.4584\n",
      "- val_f1: 0.434301 - val_precision: 0.664986 - val_recall: 0.322444\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.1259 - acc: 0.7010 - val_loss: 2.0652 - val_acc: 0.5056\n",
      "- val_f1: 0.477939 - val_precision: 0.664687 - val_recall: 0.373111\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.0880 - acc: 0.7094 - val_loss: 2.3818 - val_acc: 0.4513\n",
      "- val_f1: 0.426471 - val_precision: 0.586314 - val_recall: 0.335111\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.1431 - acc: 0.6972 - val_loss: 2.3837 - val_acc: 0.4373\n",
      "- val_f1: 0.401269 - val_precision: 0.626711 - val_recall: 0.295111\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.1645 - acc: 0.6878 - val_loss: 2.6049 - val_acc: 0.4678\n",
      "- val_f1: 0.444982 - val_precision: 0.562691 - val_recall: 0.368000\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.1484 - acc: 0.6956 - val_loss: 2.1552 - val_acc: 0.5240\n",
      "- val_f1: 0.523269 - val_precision: 0.632923 - val_recall: 0.446000\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.1675 - acc: 0.6858 - val_loss: 2.4142 - val_acc: 0.4336\n",
      "- val_f1: 0.419890 - val_precision: 0.639964 - val_recall: 0.312444\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.1579 - acc: 0.6959 - val_loss: 2.3860 - val_acc: 0.4329\n",
      "- val_f1: 0.405649 - val_precision: 0.588260 - val_recall: 0.309556\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 122s 487ms/step - loss: 1.1861 - acc: 0.6884 - val_loss: 1.6427 - val_acc: 0.5436\n",
      "- val_f1: 0.540398 - val_precision: 0.791595 - val_recall: 0.410222\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 122s 486ms/step - loss: 1.1791 - acc: 0.6929 - val_loss: 2.3616 - val_acc: 0.4618\n",
      "- val_f1: 0.455934 - val_precision: 0.616989 - val_recall: 0.361556\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.1815 - acc: 0.6838 - val_loss: 2.0247 - val_acc: 0.4698\n",
      "- val_f1: 0.437923 - val_precision: 0.678322 - val_recall: 0.323333\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.1544 - acc: 0.6978 - val_loss: 2.0901 - val_acc: 0.4740\n",
      "- val_f1: 0.455280 - val_precision: 0.648849 - val_recall: 0.350667\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.1934 - acc: 0.6877 - val_loss: 2.4590 - val_acc: 0.4427\n",
      "- val_f1: 0.426389 - val_precision: 0.596484 - val_recall: 0.331778\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.1913 - acc: 0.6849 - val_loss: 2.0323 - val_acc: 0.5202\n",
      "- val_f1: 0.515222 - val_precision: 0.667965 - val_recall: 0.419333\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.1876 - acc: 0.6857 - val_loss: 3.0186 - val_acc: 0.3687\n",
      "- val_f1: 0.324479 - val_precision: 0.505644 - val_recall: 0.238889\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.2003 - acc: 0.6844 - val_loss: 2.4650 - val_acc: 0.4493\n",
      "- val_f1: 0.440397 - val_precision: 0.580786 - val_recall: 0.354667\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.2101 - acc: 0.6863 - val_loss: 1.8646 - val_acc: 0.5344\n",
      "- val_f1: 0.523213 - val_precision: 0.688293 - val_recall: 0.422000\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 121s 486ms/step - loss: 1.2235 - acc: 0.6823 - val_loss: 1.9671 - val_acc: 0.4709\n",
      "- val_f1: 0.450546 - val_precision: 0.709847 - val_recall: 0.330000\n",
      "\n",
      "Back images training:\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 142s 568ms/step - loss: 5.5103 - acc: 0.0040 - val_loss: 5.4408 - val_acc: 0.0089\n",
      "- val_f1: 0.000000 - val_precision: 0.000000 - val_recall: 0.000000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 5.2881 - acc: 0.0094 - val_loss: 5.0353 - val_acc: 0.0149\n",
      "- val_f1: 0.003988 - val_precision: 0.692308 - val_recall: 0.002000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 4.8869 - acc: 0.0266 - val_loss: 4.5177 - val_acc: 0.0718\n",
      "- val_f1: 0.014537 - val_precision: 0.825000 - val_recall: 0.007333\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 4.4328 - acc: 0.0619 - val_loss: 4.4458 - val_acc: 0.0831\n",
      "- val_f1: 0.017177 - val_precision: 0.951220 - val_recall: 0.008667\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 3.9579 - acc: 0.1059 - val_loss: 3.7627 - val_acc: 0.1351\n",
      "- val_f1: 0.026030 - val_precision: 0.545455 - val_recall: 0.013333\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 3.5822 - acc: 0.1456 - val_loss: 3.2141 - val_acc: 0.2420\n",
      "- val_f1: 0.049647 - val_precision: 0.670520 - val_recall: 0.025778\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 3.2576 - acc: 0.1959 - val_loss: 3.1984 - val_acc: 0.2522\n",
      "- val_f1: 0.087981 - val_precision: 0.832669 - val_recall: 0.046444\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 2.9724 - acc: 0.2408 - val_loss: 2.5206 - val_acc: 0.3851\n",
      "- val_f1: 0.159546 - val_precision: 0.897494 - val_recall: 0.087556\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 2.7232 - acc: 0.2910 - val_loss: 2.2592 - val_acc: 0.4184\n",
      "- val_f1: 0.207853 - val_precision: 0.859451 - val_recall: 0.118222\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 2.5040 - acc: 0.3306 - val_loss: 2.0219 - val_acc: 0.4949\n",
      "- val_f1: 0.239938 - val_precision: 0.898990 - val_recall: 0.138444\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 2.3470 - acc: 0.3623 - val_loss: 1.7856 - val_acc: 0.5589\n",
      "- val_f1: 0.296942 - val_precision: 0.894972 - val_recall: 0.178000\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 2.1940 - acc: 0.3984 - val_loss: 1.9611 - val_acc: 0.4820\n",
      "- val_f1: 0.312831 - val_precision: 0.875383 - val_recall: 0.190444\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 2.0477 - acc: 0.4309 - val_loss: 1.6386 - val_acc: 0.5809\n",
      "- val_f1: 0.376462 - val_precision: 0.878566 - val_recall: 0.239556\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 130s 521ms/step - loss: 1.9269 - acc: 0.4602 - val_loss: 1.5331 - val_acc: 0.5898\n",
      "- val_f1: 0.427073 - val_precision: 0.895035 - val_recall: 0.280444\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 130s 521ms/step - loss: 1.8096 - acc: 0.4882 - val_loss: 1.4283 - val_acc: 0.6116\n",
      "- val_f1: 0.471276 - val_precision: 0.873646 - val_recall: 0.322667\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 130s 521ms/step - loss: 1.7130 - acc: 0.5066 - val_loss: 1.3149 - val_acc: 0.6438\n",
      "- val_f1: 0.526937 - val_precision: 0.874292 - val_recall: 0.377111\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 130s 521ms/step - loss: 1.6097 - acc: 0.5364 - val_loss: 1.6043 - val_acc: 0.5627\n",
      "- val_f1: 0.466540 - val_precision: 0.805011 - val_recall: 0.328444\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 131s 522ms/step - loss: 1.5769 - acc: 0.5437 - val_loss: 1.2129 - val_acc: 0.6767\n",
      "- val_f1: 0.555095 - val_precision: 0.899851 - val_recall: 0.401333\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 130s 521ms/step - loss: 1.4688 - acc: 0.5691 - val_loss: 1.1413 - val_acc: 0.6678\n",
      "- val_f1: 0.620025 - val_precision: 0.854744 - val_recall: 0.486444\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 1.4029 - acc: 0.5889 - val_loss: 1.1124 - val_acc: 0.6711\n",
      "- val_f1: 0.608211 - val_precision: 0.853473 - val_recall: 0.472444\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 129s 516ms/step - loss: 1.3399 - acc: 0.6042 - val_loss: 1.2215 - val_acc: 0.6511\n",
      "- val_f1: 0.594336 - val_precision: 0.857862 - val_recall: 0.454667\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 1.3098 - acc: 0.6143 - val_loss: 1.0336 - val_acc: 0.7069\n",
      "- val_f1: 0.656092 - val_precision: 0.890625 - val_recall: 0.519333\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 1.2229 - acc: 0.6349 - val_loss: 1.0019 - val_acc: 0.7060\n",
      "- val_f1: 0.687368 - val_precision: 0.847784 - val_recall: 0.578000\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 1.1996 - acc: 0.6369 - val_loss: 1.0457 - val_acc: 0.6824\n",
      "- val_f1: 0.662843 - val_precision: 0.828886 - val_recall: 0.552222\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 1.1794 - acc: 0.6498 - val_loss: 1.0768 - val_acc: 0.6856\n",
      "- val_f1: 0.644697 - val_precision: 0.855253 - val_recall: 0.517333\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 1.1035 - acc: 0.6737 - val_loss: 0.9879 - val_acc: 0.7084\n",
      "- val_f1: 0.680502 - val_precision: 0.839804 - val_recall: 0.572000\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 1.0931 - acc: 0.6737 - val_loss: 1.1181 - val_acc: 0.6764\n",
      "- val_f1: 0.665468 - val_precision: 0.788672 - val_recall: 0.575556\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 1.0467 - acc: 0.6919 - val_loss: 0.9063 - val_acc: 0.7231\n",
      "- val_f1: 0.707897 - val_precision: 0.832832 - val_recall: 0.615556\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 131s 524ms/step - loss: 1.0025 - acc: 0.6890 - val_loss: 0.9384 - val_acc: 0.7244\n",
      "- val_f1: 0.711784 - val_precision: 0.814253 - val_recall: 0.632222\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 131s 523ms/step - loss: 0.9786 - acc: 0.7060 - val_loss: 1.0026 - val_acc: 0.7129\n",
      "- val_f1: 0.691004 - val_precision: 0.835962 - val_recall: 0.588889\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 131s 522ms/step - loss: 0.9632 - acc: 0.7081 - val_loss: 0.9895 - val_acc: 0.7107\n",
      "- val_f1: 0.692746 - val_precision: 0.830435 - val_recall: 0.594222\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 131s 522ms/step - loss: 0.9569 - acc: 0.7163 - val_loss: 0.9164 - val_acc: 0.7353\n",
      "- val_f1: 0.732373 - val_precision: 0.848858 - val_recall: 0.644000\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 131s 523ms/step - loss: 0.9181 - acc: 0.7220 - val_loss: 1.1165 - val_acc: 0.6689\n",
      "- val_f1: 0.659253 - val_precision: 0.788432 - val_recall: 0.566444\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 131s 524ms/step - loss: 0.9212 - acc: 0.7217 - val_loss: 1.1948 - val_acc: 0.6633\n",
      "- val_f1: 0.658088 - val_precision: 0.781718 - val_recall: 0.568222\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 131s 524ms/step - loss: 0.8888 - acc: 0.7338 - val_loss: 0.9357 - val_acc: 0.7222\n",
      "- val_f1: 0.715441 - val_precision: 0.829233 - val_recall: 0.629111\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 131s 523ms/step - loss: 0.8859 - acc: 0.7350 - val_loss: 0.9926 - val_acc: 0.7024\n",
      "- val_f1: 0.717287 - val_precision: 0.802143 - val_recall: 0.648667\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 131s 525ms/step - loss: 0.8640 - acc: 0.7409 - val_loss: 1.0914 - val_acc: 0.6938\n",
      "- val_f1: 0.690545 - val_precision: 0.791105 - val_recall: 0.612667\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 131s 523ms/step - loss: 0.8680 - acc: 0.7450 - val_loss: 0.8993 - val_acc: 0.7418\n",
      "- val_f1: 0.745452 - val_precision: 0.813996 - val_recall: 0.687556\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 131s 525ms/step - loss: 0.8470 - acc: 0.7453 - val_loss: 1.0258 - val_acc: 0.7051\n",
      "- val_f1: 0.709717 - val_precision: 0.802410 - val_recall: 0.636222\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.8534 - acc: 0.7438 - val_loss: 0.8320 - val_acc: 0.7513\n",
      "- val_f1: 0.748609 - val_precision: 0.868545 - val_recall: 0.657778\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.8237 - acc: 0.7542 - val_loss: 0.7301 - val_acc: 0.7856\n",
      "- val_f1: 0.775440 - val_precision: 0.868762 - val_recall: 0.700222\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.8214 - acc: 0.7550 - val_loss: 1.0100 - val_acc: 0.7091\n",
      "- val_f1: 0.707279 - val_precision: 0.777216 - val_recall: 0.648889\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.8040 - acc: 0.7584 - val_loss: 0.7642 - val_acc: 0.7809\n",
      "- val_f1: 0.778702 - val_precision: 0.836995 - val_recall: 0.728000\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.8407 - acc: 0.7518 - val_loss: 0.7743 - val_acc: 0.7782\n",
      "- val_f1: 0.767820 - val_precision: 0.853493 - val_recall: 0.697778\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 0.8003 - acc: 0.7617 - val_loss: 0.7814 - val_acc: 0.7758\n",
      "- val_f1: 0.775224 - val_precision: 0.850770 - val_recall: 0.712000\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.7936 - acc: 0.7569 - val_loss: 0.8175 - val_acc: 0.7582\n",
      "- val_f1: 0.755275 - val_precision: 0.831287 - val_recall: 0.692000\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.8124 - acc: 0.7622 - val_loss: 0.9607 - val_acc: 0.7158\n",
      "- val_f1: 0.716091 - val_precision: 0.790236 - val_recall: 0.654667\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.8003 - acc: 0.7643 - val_loss: 0.7415 - val_acc: 0.7878\n",
      "- val_f1: 0.774892 - val_precision: 0.844340 - val_recall: 0.716000\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.7693 - acc: 0.7778 - val_loss: 1.0794 - val_acc: 0.6929\n",
      "- val_f1: 0.682401 - val_precision: 0.807039 - val_recall: 0.591111\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 0.7990 - acc: 0.7693 - val_loss: 0.8341 - val_acc: 0.7571\n",
      "- val_f1: 0.753314 - val_precision: 0.851302 - val_recall: 0.675556\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.7731 - acc: 0.7709 - val_loss: 0.8141 - val_acc: 0.7711\n",
      "- val_f1: 0.744722 - val_precision: 0.892580 - val_recall: 0.638889\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 0.7863 - acc: 0.7676 - val_loss: 0.8108 - val_acc: 0.7800\n",
      "- val_f1: 0.774874 - val_precision: 0.841229 - val_recall: 0.718222\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.8125 - acc: 0.7674 - val_loss: 0.9129 - val_acc: 0.7411\n",
      "- val_f1: 0.745496 - val_precision: 0.817555 - val_recall: 0.685111\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.7918 - acc: 0.7716 - val_loss: 1.1503 - val_acc: 0.6791\n",
      "- val_f1: 0.677051 - val_precision: 0.755543 - val_recall: 0.613333\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.7662 - acc: 0.7801 - val_loss: 0.6954 - val_acc: 0.8027\n",
      "- val_f1: 0.790743 - val_precision: 0.880556 - val_recall: 0.717556\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 0.7670 - acc: 0.7759 - val_loss: 0.8295 - val_acc: 0.7542\n",
      "- val_f1: 0.751392 - val_precision: 0.825093 - val_recall: 0.689778\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.7969 - acc: 0.7743 - val_loss: 0.9225 - val_acc: 0.7476\n",
      "- val_f1: 0.736558 - val_precision: 0.820464 - val_recall: 0.668222\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.7851 - acc: 0.7817 - val_loss: 0.9766 - val_acc: 0.7489\n",
      "- val_f1: 0.661597 - val_precision: 0.903114 - val_recall: 0.522000\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.7819 - acc: 0.7792 - val_loss: 0.9076 - val_acc: 0.7504\n",
      "- val_f1: 0.752506 - val_precision: 0.812629 - val_recall: 0.700667\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.7642 - acc: 0.7807 - val_loss: 1.0701 - val_acc: 0.7016\n",
      "- val_f1: 0.697417 - val_precision: 0.805531 - val_recall: 0.614889\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 129s 517ms/step - loss: 0.8019 - acc: 0.7763 - val_loss: 0.7188 - val_acc: 0.7902\n",
      "- val_f1: 0.780034 - val_precision: 0.867719 - val_recall: 0.708444\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.7628 - acc: 0.7824 - val_loss: 0.9079 - val_acc: 0.7398\n",
      "- val_f1: 0.728894 - val_precision: 0.838439 - val_recall: 0.644667\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.7472 - acc: 0.7837 - val_loss: 1.0154 - val_acc: 0.7280\n",
      "- val_f1: 0.724691 - val_precision: 0.806759 - val_recall: 0.657778\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.8032 - acc: 0.7779 - val_loss: 0.8105 - val_acc: 0.7744\n",
      "- val_f1: 0.751558 - val_precision: 0.878905 - val_recall: 0.656444\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.7820 - acc: 0.7870 - val_loss: 0.9883 - val_acc: 0.7404\n",
      "- val_f1: 0.737494 - val_precision: 0.812901 - val_recall: 0.674889\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 0.7856 - acc: 0.7814 - val_loss: 1.3375 - val_acc: 0.6336\n",
      "- val_f1: 0.625810 - val_precision: 0.750466 - val_recall: 0.536667\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.7961 - acc: 0.7802 - val_loss: 0.7394 - val_acc: 0.7931\n",
      "- val_f1: 0.798805 - val_precision: 0.864424 - val_recall: 0.742444\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 0.7929 - acc: 0.7841 - val_loss: 1.1010 - val_acc: 0.7040\n",
      "- val_f1: 0.696242 - val_precision: 0.781466 - val_recall: 0.627778\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 0.7728 - acc: 0.7831 - val_loss: 0.8365 - val_acc: 0.7622\n",
      "- val_f1: 0.745360 - val_precision: 0.862906 - val_recall: 0.656000\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 131s 526ms/step - loss: 0.8165 - acc: 0.7780 - val_loss: 0.9896 - val_acc: 0.7400\n",
      "- val_f1: 0.726509 - val_precision: 0.817449 - val_recall: 0.653778\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 131s 524ms/step - loss: 0.8218 - acc: 0.7752 - val_loss: 1.5329 - val_acc: 0.6360\n",
      "- val_f1: 0.629653 - val_precision: 0.722047 - val_recall: 0.558222\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 131s 524ms/step - loss: 0.8140 - acc: 0.7818 - val_loss: 1.1191 - val_acc: 0.6989\n",
      "- val_f1: 0.661010 - val_precision: 0.838974 - val_recall: 0.545333\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 131s 523ms/step - loss: 0.8188 - acc: 0.7840 - val_loss: 1.2694 - val_acc: 0.6560\n",
      "- val_f1: 0.631411 - val_precision: 0.785195 - val_recall: 0.528000\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 131s 524ms/step - loss: 0.8136 - acc: 0.7782 - val_loss: 0.9743 - val_acc: 0.7296\n",
      "- val_f1: 0.726698 - val_precision: 0.801824 - val_recall: 0.664444\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 134s 535ms/step - loss: 0.8501 - acc: 0.7686 - val_loss: 1.0457 - val_acc: 0.7338\n",
      "- val_f1: 0.707641 - val_precision: 0.842073 - val_recall: 0.610222\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 132s 527ms/step - loss: 0.8176 - acc: 0.7812 - val_loss: 1.0728 - val_acc: 0.7313\n",
      "- val_f1: 0.722927 - val_precision: 0.811169 - val_recall: 0.652000\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 132s 527ms/step - loss: 0.8636 - acc: 0.7726 - val_loss: 1.1076 - val_acc: 0.6984\n",
      "- val_f1: 0.684163 - val_precision: 0.807007 - val_recall: 0.593778\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 131s 526ms/step - loss: 0.8227 - acc: 0.7761 - val_loss: 1.3001 - val_acc: 0.6842\n",
      "- val_f1: 0.670360 - val_precision: 0.773388 - val_recall: 0.591556\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.8384 - acc: 0.7760 - val_loss: 1.0961 - val_acc: 0.7027\n",
      "- val_f1: 0.685951 - val_precision: 0.823933 - val_recall: 0.587556\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.8584 - acc: 0.7711 - val_loss: 1.3680 - val_acc: 0.6291\n",
      "- val_f1: 0.605987 - val_precision: 0.770576 - val_recall: 0.499333\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 0.8453 - acc: 0.7779 - val_loss: 1.2263 - val_acc: 0.6731\n",
      "- val_f1: 0.661168 - val_precision: 0.770710 - val_recall: 0.578889\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 0.8564 - acc: 0.7701 - val_loss: 1.3113 - val_acc: 0.6844\n",
      "- val_f1: 0.674512 - val_precision: 0.772362 - val_recall: 0.598667\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 0.8971 - acc: 0.7677 - val_loss: 1.3381 - val_acc: 0.6600\n",
      "- val_f1: 0.652388 - val_precision: 0.753273 - val_recall: 0.575333\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.8778 - acc: 0.7713 - val_loss: 1.3860 - val_acc: 0.6424\n",
      "- val_f1: 0.616332 - val_precision: 0.775084 - val_recall: 0.511556\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.8748 - acc: 0.7699 - val_loss: 1.0709 - val_acc: 0.7176\n",
      "- val_f1: 0.704475 - val_precision: 0.802157 - val_recall: 0.628000\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.8710 - acc: 0.7689 - val_loss: 1.8031 - val_acc: 0.5424\n",
      "- val_f1: 0.499280 - val_precision: 0.708913 - val_recall: 0.385333\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.8765 - acc: 0.7731 - val_loss: 0.9702 - val_acc: 0.7404\n",
      "- val_f1: 0.709527 - val_precision: 0.846580 - val_recall: 0.610667\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.8738 - acc: 0.7758 - val_loss: 1.2575 - val_acc: 0.6658\n",
      "- val_f1: 0.643194 - val_precision: 0.818400 - val_recall: 0.529778\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.8928 - acc: 0.7676 - val_loss: 1.3340 - val_acc: 0.6449\n",
      "- val_f1: 0.619772 - val_precision: 0.809166 - val_recall: 0.502222\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.9033 - acc: 0.7681 - val_loss: 1.2499 - val_acc: 0.6700\n",
      "- val_f1: 0.606781 - val_precision: 0.871774 - val_recall: 0.465333\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.9277 - acc: 0.7604 - val_loss: 1.3981 - val_acc: 0.6180\n",
      "- val_f1: 0.614495 - val_precision: 0.778005 - val_recall: 0.507778\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.9254 - acc: 0.7629 - val_loss: 1.0854 - val_acc: 0.7204\n",
      "- val_f1: 0.708212 - val_precision: 0.825688 - val_recall: 0.620000\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.9325 - acc: 0.7628 - val_loss: 1.4891 - val_acc: 0.6702\n",
      "- val_f1: 0.650039 - val_precision: 0.782364 - val_recall: 0.556000\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.9267 - acc: 0.7623 - val_loss: 2.5105 - val_acc: 0.4567\n",
      "- val_f1: 0.428904 - val_precision: 0.559971 - val_recall: 0.347556\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 0.9102 - acc: 0.7687 - val_loss: 1.0511 - val_acc: 0.7216\n",
      "- val_f1: 0.709096 - val_precision: 0.843654 - val_recall: 0.611556\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 0.8965 - acc: 0.7693 - val_loss: 1.3892 - val_acc: 0.6333\n",
      "- val_f1: 0.600227 - val_precision: 0.828896 - val_recall: 0.470444\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 0.9818 - acc: 0.7546 - val_loss: 1.3766 - val_acc: 0.6280\n",
      "- val_f1: 0.570107 - val_precision: 0.815550 - val_recall: 0.438222\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 0.9572 - acc: 0.7633 - val_loss: 1.8154 - val_acc: 0.5184\n",
      "- val_f1: 0.447608 - val_precision: 0.798972 - val_recall: 0.310889\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 0.9527 - acc: 0.7559 - val_loss: 1.5233 - val_acc: 0.6189\n",
      "- val_f1: 0.609737 - val_precision: 0.759443 - val_recall: 0.509333\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 0.9480 - acc: 0.7593 - val_loss: 1.2972 - val_acc: 0.6580\n",
      "- val_f1: 0.624198 - val_precision: 0.837322 - val_recall: 0.497556\n",
      "\n",
      "Highlighted images training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 110s 441ms/step - loss: 5.3054 - acc: 0.0143 - val_loss: 4.8838 - val_acc: 0.0580\n",
      "- val_f1: 0.003106 - val_precision: 1.000000 - val_recall: 0.001556\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 4.2503 - acc: 0.0879 - val_loss: 3.3262 - val_acc: 0.2798\n",
      "- val_f1: 0.095601 - val_precision: 0.991228 - val_recall: 0.050222\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 3.1674 - acc: 0.2309 - val_loss: 2.2947 - val_acc: 0.4911\n",
      "- val_f1: 0.315613 - val_precision: 0.964773 - val_recall: 0.188667\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 2.4528 - acc: 0.3623 - val_loss: 1.6300 - val_acc: 0.6631\n",
      "- val_f1: 0.504274 - val_precision: 0.968434 - val_recall: 0.340889\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 1.9382 - acc: 0.4802 - val_loss: 1.1117 - val_acc: 0.7662\n",
      "- val_f1: 0.660623 - val_precision: 0.965797 - val_recall: 0.502000\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 1.5509 - acc: 0.5689 - val_loss: 0.9528 - val_acc: 0.7778\n",
      "- val_f1: 0.705040 - val_precision: 0.957301 - val_recall: 0.558000\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 1.2565 - acc: 0.6488 - val_loss: 0.6476 - val_acc: 0.8478\n",
      "- val_f1: 0.803530 - val_precision: 0.965689 - val_recall: 0.688000\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 1.0716 - acc: 0.6947 - val_loss: 0.6239 - val_acc: 0.8424\n",
      "- val_f1: 0.829403 - val_precision: 0.952189 - val_recall: 0.734667\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.9197 - acc: 0.7307 - val_loss: 0.6506 - val_acc: 0.8380\n",
      "- val_f1: 0.826531 - val_precision: 0.939197 - val_recall: 0.738000\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.7975 - acc: 0.7660 - val_loss: 0.5511 - val_acc: 0.8529\n",
      "- val_f1: 0.858943 - val_precision: 0.944563 - val_recall: 0.787556\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.6923 - acc: 0.7933 - val_loss: 0.2989 - val_acc: 0.9264\n",
      "- val_f1: 0.926756 - val_precision: 0.970109 - val_recall: 0.887111\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.6061 - acc: 0.8158 - val_loss: 0.2149 - val_acc: 0.9444\n",
      "- val_f1: 0.945814 - val_precision: 0.967465 - val_recall: 0.925111\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.5776 - acc: 0.8293 - val_loss: 0.2010 - val_acc: 0.9493\n",
      "- val_f1: 0.947823 - val_precision: 0.970212 - val_recall: 0.926444\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.5339 - acc: 0.8439 - val_loss: 0.1814 - val_acc: 0.9518\n",
      "- val_f1: 0.952435 - val_precision: 0.971132 - val_recall: 0.934444\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.4826 - acc: 0.8576 - val_loss: 0.1574 - val_acc: 0.9573\n",
      "- val_f1: 0.958150 - val_precision: 0.972967 - val_recall: 0.943778\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.4421 - acc: 0.8671 - val_loss: 0.2519 - val_acc: 0.9287\n",
      "- val_f1: 0.928970 - val_precision: 0.953875 - val_recall: 0.905333\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.4147 - acc: 0.8754 - val_loss: 0.2553 - val_acc: 0.9236\n",
      "- val_f1: 0.922380 - val_precision: 0.941040 - val_recall: 0.904444\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.3936 - acc: 0.8820 - val_loss: 0.1341 - val_acc: 0.9640\n",
      "- val_f1: 0.967003 - val_precision: 0.976871 - val_recall: 0.957333\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.3830 - acc: 0.8861 - val_loss: 0.1004 - val_acc: 0.9727\n",
      "- val_f1: 0.973469 - val_precision: 0.980826 - val_recall: 0.966222\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.3465 - acc: 0.8990 - val_loss: 0.1529 - val_acc: 0.9556\n",
      "- val_f1: 0.958259 - val_precision: 0.967815 - val_recall: 0.948889\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.3117 - acc: 0.9009 - val_loss: 0.0898 - val_acc: 0.9758\n",
      "- val_f1: 0.977465 - val_precision: 0.981407 - val_recall: 0.973556\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.3030 - acc: 0.9086 - val_loss: 0.1568 - val_acc: 0.9571\n",
      "- val_f1: 0.959713 - val_precision: 0.969395 - val_recall: 0.950222\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.3105 - acc: 0.9044 - val_loss: 0.1748 - val_acc: 0.9493\n",
      "- val_f1: 0.951504 - val_precision: 0.961434 - val_recall: 0.941778\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.3034 - acc: 0.9100 - val_loss: 0.1460 - val_acc: 0.9549\n",
      "- val_f1: 0.955317 - val_precision: 0.960467 - val_recall: 0.950222\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2853 - acc: 0.9172 - val_loss: 0.3106 - val_acc: 0.9209\n",
      "- val_f1: 0.921659 - val_precision: 0.932454 - val_recall: 0.911111\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 104s 414ms/step - loss: 0.2808 - acc: 0.9183 - val_loss: 0.1353 - val_acc: 0.9616\n",
      "- val_f1: 0.960582 - val_precision: 0.968172 - val_recall: 0.953111\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.2459 - acc: 0.9274 - val_loss: 0.1052 - val_acc: 0.9702\n",
      "- val_f1: 0.972123 - val_precision: 0.975604 - val_recall: 0.968667\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 101s 405ms/step - loss: 0.2686 - acc: 0.9212 - val_loss: 0.1901 - val_acc: 0.9460\n",
      "- val_f1: 0.949005 - val_precision: 0.955200 - val_recall: 0.942889\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 101s 405ms/step - loss: 0.2424 - acc: 0.9284 - val_loss: 0.1687 - val_acc: 0.9644\n",
      "- val_f1: 0.966240 - val_precision: 0.968939 - val_recall: 0.963556\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.2422 - acc: 0.9294 - val_loss: 0.1188 - val_acc: 0.9687\n",
      "- val_f1: 0.969744 - val_precision: 0.974422 - val_recall: 0.965111\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 101s 403ms/step - loss: 0.2352 - acc: 0.9327 - val_loss: 0.2251 - val_acc: 0.9331\n",
      "- val_f1: 0.936456 - val_precision: 0.944608 - val_recall: 0.928444\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 100s 402ms/step - loss: 0.2235 - acc: 0.9326 - val_loss: 0.0921 - val_acc: 0.9791\n",
      "- val_f1: 0.979933 - val_precision: 0.983221 - val_recall: 0.976667\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2357 - acc: 0.9340 - val_loss: 0.1045 - val_acc: 0.9731\n",
      "- val_f1: 0.973981 - val_precision: 0.978900 - val_recall: 0.969111\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.2226 - acc: 0.9362 - val_loss: 0.0950 - val_acc: 0.9758\n",
      "- val_f1: 0.977599 - val_precision: 0.980550 - val_recall: 0.974667\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2260 - acc: 0.9371 - val_loss: 0.0734 - val_acc: 0.9813\n",
      "- val_f1: 0.980414 - val_precision: 0.981944 - val_recall: 0.978889\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2135 - acc: 0.9374 - val_loss: 0.1460 - val_acc: 0.9676\n",
      "- val_f1: 0.967577 - val_precision: 0.970279 - val_recall: 0.964889\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2137 - acc: 0.9370 - val_loss: 0.3802 - val_acc: 0.9256\n",
      "- val_f1: 0.928148 - val_precision: 0.933468 - val_recall: 0.922889\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.2061 - acc: 0.9386 - val_loss: 0.2536 - val_acc: 0.9467\n",
      "- val_f1: 0.945572 - val_precision: 0.949172 - val_recall: 0.942000\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2147 - acc: 0.9412 - val_loss: 0.1056 - val_acc: 0.9751\n",
      "- val_f1: 0.975610 - val_precision: 0.977897 - val_recall: 0.973333\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2054 - acc: 0.9411 - val_loss: 0.0888 - val_acc: 0.9762\n",
      "- val_f1: 0.977753 - val_precision: 0.978842 - val_recall: 0.976667\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2225 - acc: 0.9378 - val_loss: 0.1210 - val_acc: 0.9724\n",
      "- val_f1: 0.973039 - val_precision: 0.975648 - val_recall: 0.970444\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2182 - acc: 0.9381 - val_loss: 0.4087 - val_acc: 0.9156\n",
      "- val_f1: 0.921547 - val_precision: 0.926933 - val_recall: 0.916222\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.1971 - acc: 0.9446 - val_loss: 0.2343 - val_acc: 0.9540\n",
      "- val_f1: 0.954788 - val_precision: 0.956920 - val_recall: 0.952667\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2100 - acc: 0.9417 - val_loss: 0.4618 - val_acc: 0.9078\n",
      "- val_f1: 0.911571 - val_precision: 0.917210 - val_recall: 0.906000\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2062 - acc: 0.9429 - val_loss: 0.1734 - val_acc: 0.9647\n",
      "- val_f1: 0.965217 - val_precision: 0.968456 - val_recall: 0.962000\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.1881 - acc: 0.9473 - val_loss: 0.0772 - val_acc: 0.9816\n",
      "- val_f1: 0.981409 - val_precision: 0.983270 - val_recall: 0.979556\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2057 - acc: 0.9446 - val_loss: 0.0793 - val_acc: 0.9809\n",
      "- val_f1: 0.979188 - val_precision: 0.980825 - val_recall: 0.977556\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.1985 - acc: 0.9487 - val_loss: 0.1308 - val_acc: 0.9736\n",
      "- val_f1: 0.974062 - val_precision: 0.975909 - val_recall: 0.972222\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.1936 - acc: 0.9467 - val_loss: 0.4452 - val_acc: 0.9129\n",
      "- val_f1: 0.913533 - val_precision: 0.917320 - val_recall: 0.909778\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2118 - acc: 0.9432 - val_loss: 0.1268 - val_acc: 0.9718\n",
      "- val_f1: 0.970729 - val_precision: 0.972352 - val_recall: 0.969111\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2018 - acc: 0.9466 - val_loss: 0.3308 - val_acc: 0.9396\n",
      "- val_f1: 0.938434 - val_precision: 0.942006 - val_recall: 0.934889\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2063 - acc: 0.9460 - val_loss: 0.1198 - val_acc: 0.9733\n",
      "- val_f1: 0.975186 - val_precision: 0.976599 - val_recall: 0.973778\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.1940 - acc: 0.9454 - val_loss: 0.3000 - val_acc: 0.9487\n",
      "- val_f1: 0.949760 - val_precision: 0.952200 - val_recall: 0.947333\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2186 - acc: 0.9444 - val_loss: 0.1081 - val_acc: 0.9802\n",
      "- val_f1: 0.979556 - val_precision: 0.979556 - val_recall: 0.979556\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2109 - acc: 0.9467 - val_loss: 0.1008 - val_acc: 0.9769\n",
      "- val_f1: 0.978539 - val_precision: 0.979301 - val_recall: 0.977778\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.1950 - acc: 0.9490 - val_loss: 0.1716 - val_acc: 0.9651\n",
      "- val_f1: 0.967060 - val_precision: 0.968569 - val_recall: 0.965556\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2107 - acc: 0.9453 - val_loss: 0.2079 - val_acc: 0.9656\n",
      "- val_f1: 0.966277 - val_precision: 0.967893 - val_recall: 0.964667\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2079 - acc: 0.9423 - val_loss: 0.0992 - val_acc: 0.9798\n",
      "- val_f1: 0.980183 - val_precision: 0.982151 - val_recall: 0.978222\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2068 - acc: 0.9471 - val_loss: 0.1708 - val_acc: 0.9669\n",
      "- val_f1: 0.968173 - val_precision: 0.969683 - val_recall: 0.966667\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2139 - acc: 0.9472 - val_loss: 0.1689 - val_acc: 0.9664\n",
      "- val_f1: 0.966285 - val_precision: 0.967684 - val_recall: 0.964889\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2104 - acc: 0.9472 - val_loss: 0.1475 - val_acc: 0.9767\n",
      "- val_f1: 0.974844 - val_precision: 0.976583 - val_recall: 0.973111\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.2165 - acc: 0.9474 - val_loss: 0.0868 - val_acc: 0.9844\n",
      "- val_f1: 0.985324 - val_precision: 0.985981 - val_recall: 0.984667\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2184 - acc: 0.9459 - val_loss: 0.1047 - val_acc: 0.9736\n",
      "- val_f1: 0.975512 - val_precision: 0.977252 - val_recall: 0.973778\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2060 - acc: 0.9496 - val_loss: 0.2430 - val_acc: 0.9598\n",
      "- val_f1: 0.960828 - val_precision: 0.962327 - val_recall: 0.959333\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2041 - acc: 0.9490 - val_loss: 0.3242 - val_acc: 0.9293\n",
      "- val_f1: 0.931069 - val_precision: 0.936194 - val_recall: 0.926000\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2011 - acc: 0.9489 - val_loss: 0.2960 - val_acc: 0.9489\n",
      "- val_f1: 0.951320 - val_precision: 0.953764 - val_recall: 0.948889\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2193 - acc: 0.9464 - val_loss: 0.0937 - val_acc: 0.9771\n",
      "- val_f1: 0.977574 - val_precision: 0.981627 - val_recall: 0.973556\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2178 - acc: 0.9502 - val_loss: 0.1737 - val_acc: 0.9704\n",
      "- val_f1: 0.968827 - val_precision: 0.970772 - val_recall: 0.966889\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2495 - acc: 0.9414 - val_loss: 0.1245 - val_acc: 0.9762\n",
      "- val_f1: 0.974473 - val_precision: 0.977634 - val_recall: 0.971333\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2354 - acc: 0.9446 - val_loss: 0.2567 - val_acc: 0.9551\n",
      "- val_f1: 0.956231 - val_precision: 0.958473 - val_recall: 0.954000\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2196 - acc: 0.9490 - val_loss: 0.1064 - val_acc: 0.9769\n",
      "- val_f1: 0.977164 - val_precision: 0.979674 - val_recall: 0.974667\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2346 - acc: 0.9458 - val_loss: 0.2037 - val_acc: 0.9653\n",
      "- val_f1: 0.963598 - val_precision: 0.965425 - val_recall: 0.961778\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2322 - acc: 0.9466 - val_loss: 0.1592 - val_acc: 0.9731\n",
      "- val_f1: 0.967929 - val_precision: 0.970089 - val_recall: 0.965778\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2247 - acc: 0.9503 - val_loss: 0.2495 - val_acc: 0.9531\n",
      "- val_f1: 0.953507 - val_precision: 0.956814 - val_recall: 0.950222\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.1884 - acc: 0.9540 - val_loss: 0.2869 - val_acc: 0.9593\n",
      "- val_f1: 0.961299 - val_precision: 0.962155 - val_recall: 0.960444\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.2094 - acc: 0.9534 - val_loss: 0.2260 - val_acc: 0.9611\n",
      "- val_f1: 0.963837 - val_precision: 0.965233 - val_recall: 0.962444\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2198 - acc: 0.9492 - val_loss: 0.1470 - val_acc: 0.9724\n",
      "- val_f1: 0.972473 - val_precision: 0.975408 - val_recall: 0.969556\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2376 - acc: 0.9506 - val_loss: 0.1300 - val_acc: 0.9751\n",
      "- val_f1: 0.973532 - val_precision: 0.974399 - val_recall: 0.972667\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2245 - acc: 0.9489 - val_loss: 0.3862 - val_acc: 0.9487\n",
      "- val_f1: 0.949722 - val_precision: 0.950780 - val_recall: 0.948667\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2100 - acc: 0.9529 - val_loss: 0.1853 - val_acc: 0.9587\n",
      "- val_f1: 0.963344 - val_precision: 0.968975 - val_recall: 0.957778\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2364 - acc: 0.9494 - val_loss: 0.4783 - val_acc: 0.9431\n",
      "- val_f1: 0.944420 - val_precision: 0.944840 - val_recall: 0.944000\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.2381 - acc: 0.9493 - val_loss: 0.3047 - val_acc: 0.9487\n",
      "- val_f1: 0.951689 - val_precision: 0.957902 - val_recall: 0.945556\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2520 - acc: 0.9433 - val_loss: 0.1758 - val_acc: 0.9716\n",
      "- val_f1: 0.968726 - val_precision: 0.970346 - val_recall: 0.967111\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2336 - acc: 0.9528 - val_loss: 0.0956 - val_acc: 0.9864\n",
      "- val_f1: 0.985662 - val_precision: 0.985991 - val_recall: 0.985333\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2482 - acc: 0.9458 - val_loss: 0.9307 - val_acc: 0.8916\n",
      "- val_f1: 0.892798 - val_precision: 0.894490 - val_recall: 0.891111\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2370 - acc: 0.9497 - val_loss: 0.0607 - val_acc: 0.9878\n",
      "- val_f1: 0.987082 - val_precision: 0.989286 - val_recall: 0.984889\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2421 - acc: 0.9511 - val_loss: 0.1079 - val_acc: 0.9773\n",
      "- val_f1: 0.980091 - val_precision: 0.981073 - val_recall: 0.979111\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.2140 - acc: 0.9520 - val_loss: 0.1516 - val_acc: 0.9736\n",
      "- val_f1: 0.973382 - val_precision: 0.975664 - val_recall: 0.971111\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2371 - acc: 0.9497 - val_loss: 0.5103 - val_acc: 0.9351\n",
      "- val_f1: 0.935053 - val_precision: 0.935886 - val_recall: 0.934222\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2338 - acc: 0.9540 - val_loss: 0.2998 - val_acc: 0.9622\n",
      "- val_f1: 0.964206 - val_precision: 0.964635 - val_recall: 0.963778\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2537 - acc: 0.9477 - val_loss: 0.4768 - val_acc: 0.9304\n",
      "- val_f1: 0.933363 - val_precision: 0.936075 - val_recall: 0.930667\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2380 - acc: 0.9500 - val_loss: 0.2169 - val_acc: 0.9738\n",
      "- val_f1: 0.973772 - val_precision: 0.973988 - val_recall: 0.973556\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2554 - acc: 0.9472 - val_loss: 0.8927 - val_acc: 0.9091\n",
      "- val_f1: 0.905631 - val_precision: 0.907044 - val_recall: 0.904222\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2633 - acc: 0.9478 - val_loss: 0.1733 - val_acc: 0.9769\n",
      "- val_f1: 0.977323 - val_precision: 0.977758 - val_recall: 0.976889\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2443 - acc: 0.9492 - val_loss: 0.7456 - val_acc: 0.9091\n",
      "- val_f1: 0.910104 - val_precision: 0.912441 - val_recall: 0.907778\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2558 - acc: 0.9489 - val_loss: 0.7716 - val_acc: 0.9173\n",
      "- val_f1: 0.915545 - val_precision: 0.916871 - val_recall: 0.914222\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2938 - acc: 0.9417 - val_loss: 0.2001 - val_acc: 0.9569\n",
      "- val_f1: 0.952477 - val_precision: 0.963190 - val_recall: 0.942000\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2727 - acc: 0.9493 - val_loss: 0.1388 - val_acc: 0.9780\n",
      "- val_f1: 0.974433 - val_precision: 0.974867 - val_recall: 0.974000\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2660 - acc: 0.9518 - val_loss: 0.3411 - val_acc: 0.9544\n",
      "- val_f1: 0.951385 - val_precision: 0.952551 - val_recall: 0.950222\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2984 - acc: 0.9428 - val_loss: 0.6968 - val_acc: 0.9073\n",
      "- val_f1: 0.907329 - val_precision: 0.909558 - val_recall: 0.905111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f568b7e8a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Front images training:\")\n",
    "\n",
    "model_front.fit_generator(\n",
    "    front_train_gen,\n",
    "    steps_per_epoch = 250,\n",
    "    epochs = 100,\n",
    "    validation_data = front_validation_gen,\n",
    "    validation_steps = 125,\n",
    "    callbacks = [MetricsFront()])\n",
    "\n",
    "print(\"\\nBack images training:\")\n",
    "\n",
    "model_back.fit_generator(\n",
    "    back_train_gen,\n",
    "    steps_per_epoch = 250,\n",
    "    epochs = 100,\n",
    "    validation_data = back_validation_gen,\n",
    "    validation_steps = 125,\n",
    "    callbacks = [MetricsBack()])\n",
    "\n",
    "print(\"\\nHighlighted images training:\")\n",
    "\n",
    "model_high.fit_generator(\n",
    "    high_train_gen,\n",
    "    steps_per_epoch = 250,\n",
    "    epochs = 100,\n",
    "    validation_data = high_validation_gen,\n",
    "    validation_steps = 125,\n",
    "    callbacks = [MetricsHigh()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. 尋找最佳模型並繪製 F1 Score 趨勢圖 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Front]\t\t42 epochs reach the best f1-score value 65.35%\n",
      "[Back]\t\t67 epochs reach the best f1-score value 79.88%\n",
      "[Highlighted]\t86 epochs reach the best f1-score value 98.71%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_front_epochs = front_f1s.index(max(front_f1s)) + 1\n",
    "best_back_epochs = back_f1s.index(max(back_f1s)) + 1\n",
    "best_high_epochs = high_f1s.index(max(high_f1s)) + 1\n",
    "\n",
    "print(\"[Front]\\t\\t%d epochs reach the best f1-score value %.2f%%\"%(best_front_epochs, max(front_f1s) * 100))\n",
    "print(\"[Back]\\t\\t%d epochs reach the best f1-score value %.2f%%\"%(best_back_epochs, max(back_f1s) * 100))\n",
    "print(\"[Highlighted]\\t%d epochs reach the best f1-score value %.2f%%\"%(best_high_epochs, max(high_f1s) * 100))\n",
    "\n",
    "plt.plot(range(100), front_f1s, 'b-o', label = 'Front images')\n",
    "plt.plot(range(100), back_f1s, 'g-o', label = 'Back images')\n",
    "plt.plot(range(100), high_f1s, 'r-o', label = 'Highlighted images')\n",
    "plt.legend()\n",
    "plt.title('Validation F1-score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(0, 100, step = 10), range(1, 101, 10))\n",
    "plt.ylabel('F1-score')\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. 使用完整資料重新訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of front images:\n",
      "[Training set]\n",
      "Found 13500 images belonging to 250 classes.\n",
      "[Testing set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Status of back images:\n",
      "[Training set]\n",
      "Found 13500 images belonging to 250 classes.\n",
      "[Testing set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Status of highlighted images:\n",
      "[Training set]\n",
      "Found 13500 images belonging to 250 classes.\n",
      "[Testing set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Front images training:\n",
      "Epoch 1/42\n",
      "375/375 [==============================] - 135s 360ms/step - loss: 5.4215 - acc: 0.0064\n",
      "Epoch 2/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 4.9581 - acc: 0.0263\n",
      "Epoch 3/42\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 4.3009 - acc: 0.0824\n",
      "Epoch 4/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 3.7452 - acc: 0.1466\n",
      "Epoch 5/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 3.3085 - acc: 0.2057\n",
      "Epoch 6/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 2.9500 - acc: 0.2607\n",
      "Epoch 7/42\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 2.6346 - acc: 0.3164\n",
      "Epoch 8/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 2.3793 - acc: 0.3669\n",
      "Epoch 9/42\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 2.1756 - acc: 0.4070\n",
      "Epoch 10/42\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 2.0420 - acc: 0.4384\n",
      "Epoch 11/42\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 1.8742 - acc: 0.4733\n",
      "Epoch 12/42\n",
      "375/375 [==============================] - 135s 361ms/step - loss: 1.8010 - acc: 0.4911\n",
      "Epoch 13/42\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 1.7252 - acc: 0.5068\n",
      "Epoch 14/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 1.6402 - acc: 0.5287\n",
      "Epoch 15/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 1.5445 - acc: 0.5492\n",
      "Epoch 16/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 1.5019 - acc: 0.5656\n",
      "Epoch 17/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 1.4635 - acc: 0.5696\n",
      "Epoch 18/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 1.4098 - acc: 0.5905\n",
      "Epoch 19/42\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 1.3606 - acc: 0.6067\n",
      "Epoch 20/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.3346 - acc: 0.6083\n",
      "Epoch 21/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.3029 - acc: 0.6178\n",
      "Epoch 22/42\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 1.2680 - acc: 0.6226\n",
      "Epoch 23/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.2462 - acc: 0.6331\n",
      "Epoch 24/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.2235 - acc: 0.6410\n",
      "Epoch 25/42\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 1.2000 - acc: 0.6479\n",
      "Epoch 26/42\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 1.1777 - acc: 0.6555\n",
      "Epoch 27/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1631 - acc: 0.6613\n",
      "Epoch 28/42\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 1.1639 - acc: 0.6576\n",
      "Epoch 29/42\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 1.1349 - acc: 0.6687\n",
      "Epoch 30/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1333 - acc: 0.6688\n",
      "Epoch 31/42\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 1.1262 - acc: 0.6713\n",
      "Epoch 32/42\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 1.1281 - acc: 0.6709\n",
      "Epoch 33/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1121 - acc: 0.6806\n",
      "Epoch 34/42\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 1.1095 - acc: 0.6769\n",
      "Epoch 35/42\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 1.1013 - acc: 0.6833\n",
      "Epoch 36/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1046 - acc: 0.6776\n",
      "Epoch 37/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.0850 - acc: 0.6830\n",
      "Epoch 38/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1022 - acc: 0.6827\n",
      "Epoch 39/42\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 1.1081 - acc: 0.6871\n",
      "Epoch 40/42\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 1.0652 - acc: 0.6904\n",
      "Epoch 41/42\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1018 - acc: 0.6893\n",
      "Epoch 42/42\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 1.1032 - acc: 0.6907\n",
      "\n",
      "Back images training:\n",
      "Epoch 1/67\n",
      "375/375 [==============================] - 144s 384ms/step - loss: 5.5033 - acc: 0.0053\n",
      "Epoch 2/67\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 5.0968 - acc: 0.0200\n",
      "Epoch 3/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 4.3440 - acc: 0.0650\n",
      "Epoch 4/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 3.7410 - acc: 0.1212\n",
      "Epoch 5/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 3.2910 - acc: 0.1784\n",
      "Epoch 6/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 2.9301 - acc: 0.2414\n",
      "Epoch 7/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 2.6396 - acc: 0.2966\n",
      "Epoch 8/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 2.4024 - acc: 0.3493\n",
      "Epoch 9/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 2.2104 - acc: 0.3921\n",
      "Epoch 10/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 2.0356 - acc: 0.4288\n",
      "Epoch 11/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.8781 - acc: 0.4638\n",
      "Epoch 12/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 1.7531 - acc: 0.4964\n",
      "Epoch 13/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 1.6491 - acc: 0.5244\n",
      "Epoch 14/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.5435 - acc: 0.5455\n",
      "Epoch 15/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.4856 - acc: 0.5656\n",
      "Epoch 16/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.3826 - acc: 0.5891\n",
      "Epoch 17/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.3400 - acc: 0.6063\n",
      "Epoch 18/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.2751 - acc: 0.6254\n",
      "Epoch 19/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.2186 - acc: 0.6376\n",
      "Epoch 20/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.1806 - acc: 0.6456\n",
      "Epoch 21/67\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 1.1454 - acc: 0.6556\n",
      "Epoch 22/67\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 1.0918 - acc: 0.6703\n",
      "Epoch 23/67\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 1.0823 - acc: 0.6792\n",
      "Epoch 24/67\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 1.0162 - acc: 0.6927\n",
      "Epoch 25/67\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 1.0142 - acc: 0.7010\n",
      "Epoch 26/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.9809 - acc: 0.7087\n",
      "Epoch 27/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.9632 - acc: 0.7094\n",
      "Epoch 28/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9627 - acc: 0.7174\n",
      "Epoch 29/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9364 - acc: 0.7225\n",
      "Epoch 30/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.9176 - acc: 0.7276\n",
      "Epoch 31/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.9013 - acc: 0.7314\n",
      "Epoch 32/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.8718 - acc: 0.7433\n",
      "Epoch 33/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.8696 - acc: 0.7358\n",
      "Epoch 34/67\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 0.8638 - acc: 0.7487\n",
      "Epoch 35/67\n",
      "375/375 [==============================] - 142s 379ms/step - loss: 0.8911 - acc: 0.7402\n",
      "Epoch 36/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.8561 - acc: 0.7458\n",
      "Epoch 37/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.8648 - acc: 0.7503\n",
      "Epoch 38/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.8508 - acc: 0.7524\n",
      "Epoch 39/67\n",
      "375/375 [==============================] - 142s 377ms/step - loss: 0.8567 - acc: 0.7554\n",
      "Epoch 40/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.8554 - acc: 0.7539\n",
      "Epoch 41/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.8691 - acc: 0.7531\n",
      "Epoch 42/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.8771 - acc: 0.7522\n",
      "Epoch 43/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.8461 - acc: 0.7581\n",
      "Epoch 44/67\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 0.8441 - acc: 0.7578\n",
      "Epoch 45/67\n",
      "375/375 [==============================] - 142s 377ms/step - loss: 0.8666 - acc: 0.7594\n",
      "Epoch 46/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.8729 - acc: 0.7598\n",
      "Epoch 47/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.8866 - acc: 0.7575\n",
      "Epoch 48/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.8857 - acc: 0.7607\n",
      "Epoch 49/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9053 - acc: 0.7575\n",
      "Epoch 50/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9128 - acc: 0.7545\n",
      "Epoch 51/67\n",
      "375/375 [==============================] - 142s 377ms/step - loss: 0.9262 - acc: 0.7524\n",
      "Epoch 52/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.9167 - acc: 0.7517\n",
      "Epoch 53/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 0.9188 - acc: 0.7520\n",
      "Epoch 54/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9461 - acc: 0.7473\n",
      "Epoch 55/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9657 - acc: 0.7389\n",
      "Epoch 56/67\n",
      "375/375 [==============================] - 142s 377ms/step - loss: 0.9500 - acc: 0.7530\n",
      "Epoch 57/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9563 - acc: 0.7460\n",
      "Epoch 58/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9884 - acc: 0.7450\n",
      "Epoch 59/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9917 - acc: 0.7436\n",
      "Epoch 60/67\n",
      "375/375 [==============================] - 142s 377ms/step - loss: 0.9987 - acc: 0.7414\n",
      "Epoch 61/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 1.0141 - acc: 0.7390\n",
      "Epoch 62/67\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 1.0112 - acc: 0.7422\n",
      "Epoch 63/67\n",
      "375/375 [==============================] - 142s 378ms/step - loss: 1.0718 - acc: 0.7305\n",
      "Epoch 64/67\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.0534 - acc: 0.7341\n",
      "Epoch 65/67\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.0422 - acc: 0.7396\n",
      "Epoch 66/67\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.0538 - acc: 0.7361\n",
      "Epoch 67/67\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.0790 - acc: 0.7282\n",
      "\n",
      "Highlighted images training:\n",
      "Epoch 1/86\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 5.2160 - acc: 0.0206\n",
      "Epoch 2/86\n",
      "375/375 [==============================] - 110s 292ms/step - loss: 3.9753 - acc: 0.1398\n",
      "Epoch 3/86\n",
      "375/375 [==============================] - 110s 292ms/step - loss: 2.8809 - acc: 0.3032\n",
      "Epoch 4/86\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 2.1075 - acc: 0.4564\n",
      "Epoch 5/86\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 1.5977 - acc: 0.5645\n",
      "Epoch 6/86\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 1.2671 - acc: 0.6450\n",
      "Epoch 7/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 1.0266 - acc: 0.7097\n",
      "Epoch 8/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.8641 - acc: 0.7514\n",
      "Epoch 9/86\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.7329 - acc: 0.7900\n",
      "Epoch 10/86\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.6145 - acc: 0.8168\n",
      "Epoch 11/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.5571 - acc: 0.8353\n",
      "Epoch 12/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.5071 - acc: 0.8488\n",
      "Epoch 13/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.4515 - acc: 0.8621\n",
      "Epoch 14/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.4177 - acc: 0.8750\n",
      "Epoch 15/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.3844 - acc: 0.8833\n",
      "Epoch 16/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.3331 - acc: 0.8970\n",
      "Epoch 17/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.3353 - acc: 0.9020\n",
      "Epoch 18/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.3004 - acc: 0.9104\n",
      "Epoch 19/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2908 - acc: 0.9130\n",
      "Epoch 20/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2744 - acc: 0.9193\n",
      "Epoch 21/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2662 - acc: 0.9209\n",
      "Epoch 22/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2593 - acc: 0.9241\n",
      "Epoch 23/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2483 - acc: 0.9290\n",
      "Epoch 24/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2531 - acc: 0.9299\n",
      "Epoch 25/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2376 - acc: 0.9311\n",
      "Epoch 26/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2205 - acc: 0.9319\n",
      "Epoch 27/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2289 - acc: 0.9333\n",
      "Epoch 28/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2227 - acc: 0.9364\n",
      "Epoch 29/86\n",
      "375/375 [==============================] - 110s 292ms/step - loss: 0.2063 - acc: 0.9403\n",
      "Epoch 30/86\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2120 - acc: 0.9391\n",
      "Epoch 31/86\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2108 - acc: 0.9421\n",
      "Epoch 32/86\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2169 - acc: 0.9390\n",
      "Epoch 33/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2116 - acc: 0.9419\n",
      "Epoch 34/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.1993 - acc: 0.9416\n",
      "Epoch 35/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2135 - acc: 0.9387\n",
      "Epoch 36/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2155 - acc: 0.9413\n",
      "Epoch 37/86\n",
      "375/375 [==============================] - 111s 295ms/step - loss: 0.2013 - acc: 0.9436\n",
      "Epoch 38/86\n",
      "375/375 [==============================] - 110s 293ms/step - loss: 0.2069 - acc: 0.9444\n",
      "Epoch 39/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2126 - acc: 0.9446\n",
      "Epoch 40/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2178 - acc: 0.9433\n",
      "Epoch 41/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.1950 - acc: 0.9475\n",
      "Epoch 42/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2186 - acc: 0.9466\n",
      "Epoch 43/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2190 - acc: 0.9440\n",
      "Epoch 44/86\n",
      "375/375 [==============================] - 111s 296ms/step - loss: 0.2132 - acc: 0.9459\n",
      "Epoch 45/86\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 0.2135 - acc: 0.9456\n",
      "Epoch 46/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2136 - acc: 0.9464\n",
      "Epoch 47/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2153 - acc: 0.9476\n",
      "Epoch 48/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2270 - acc: 0.9423\n",
      "Epoch 49/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2342 - acc: 0.9459\n",
      "Epoch 50/86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2193 - acc: 0.9490\n",
      "Epoch 51/86\n",
      "375/375 [==============================] - 109s 289ms/step - loss: 0.2358 - acc: 0.9438\n",
      "Epoch 52/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2279 - acc: 0.9467\n",
      "Epoch 53/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2332 - acc: 0.9473\n",
      "Epoch 54/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2411 - acc: 0.9447\n",
      "Epoch 55/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2397 - acc: 0.9450\n",
      "Epoch 56/86\n",
      "375/375 [==============================] - 109s 289ms/step - loss: 0.2586 - acc: 0.9449\n",
      "Epoch 57/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2573 - acc: 0.9433\n",
      "Epoch 58/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2557 - acc: 0.9439\n",
      "Epoch 59/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2674 - acc: 0.9423\n",
      "Epoch 60/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2512 - acc: 0.9447\n",
      "Epoch 61/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2490 - acc: 0.9467\n",
      "Epoch 62/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2839 - acc: 0.9423\n",
      "Epoch 63/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2613 - acc: 0.9449\n",
      "Epoch 64/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2450 - acc: 0.9487\n",
      "Epoch 65/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2613 - acc: 0.9473\n",
      "Epoch 66/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2656 - acc: 0.9467\n",
      "Epoch 67/86\n",
      "375/375 [==============================] - 108s 289ms/step - loss: 0.2578 - acc: 0.9473\n",
      "Epoch 68/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2645 - acc: 0.9484\n",
      "Epoch 69/86\n",
      "375/375 [==============================] - 109s 289ms/step - loss: 0.2635 - acc: 0.9452\n",
      "Epoch 70/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2560 - acc: 0.9490\n",
      "Epoch 71/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2473 - acc: 0.9491\n",
      "Epoch 72/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2693 - acc: 0.9493\n",
      "Epoch 73/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.3086 - acc: 0.9428\n",
      "Epoch 74/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2875 - acc: 0.9451\n",
      "Epoch 75/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2962 - acc: 0.9421\n",
      "Epoch 76/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.3003 - acc: 0.9431\n",
      "Epoch 77/86\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2970 - acc: 0.9476\n",
      "Epoch 78/86\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2812 - acc: 0.9490\n",
      "Epoch 79/86\n",
      "375/375 [==============================] - 115s 308ms/step - loss: 0.2900 - acc: 0.9475\n",
      "Epoch 80/86\n",
      "375/375 [==============================] - 113s 302ms/step - loss: 0.2699 - acc: 0.9525\n",
      "Epoch 81/86\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 0.3178 - acc: 0.9462\n",
      "Epoch 82/86\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 0.3141 - acc: 0.9481\n",
      "Epoch 83/86\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 0.3167 - acc: 0.9469\n",
      "Epoch 84/86\n",
      "375/375 [==============================] - 110s 293ms/step - loss: 0.3222 - acc: 0.9479\n",
      "Epoch 85/86\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 0.3089 - acc: 0.9483\n",
      "Epoch 86/86\n",
      "375/375 [==============================] - 110s 293ms/step - loss: 0.2819 - acc: 0.9523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55d46d1f60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新建立卷積神經網路\n",
    "model_front = models.Sequential()\n",
    "model_front.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Flatten())\n",
    "model_front.add(layers.Dense(128, activation = 'relu'))\n",
    "model_front.add(Dropout(0.5))\n",
    "model_front.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "model_back = models.Sequential()\n",
    "model_back.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Flatten())\n",
    "model_back.add(layers.Dense(128, activation = 'relu'))\n",
    "model_back.add(Dropout(0.5))\n",
    "model_back.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "model_high = models.Sequential()\n",
    "model_high.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Flatten())\n",
    "model_high.add(layers.Dense(128, activation = 'relu'))\n",
    "model_high.add(Dropout(0.5))\n",
    "model_high.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "# 重新編譯模型\n",
    "model_front.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "model_back.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "model_high.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# 重新製作數據 Generator\n",
    "data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"Status of front images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "front_train_gen = data_generator.flow_from_directory(\n",
    "        front_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"[Testing set]\")\n",
    "\n",
    "front_test_gen = data_generator.flow_from_directory(\n",
    "        front_test_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"\\nStatus of back images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "back_train_gen = data_generator.flow_from_directory(\n",
    "        back_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"[Testing set]\")\n",
    "\n",
    "back_test_gen = data_generator.flow_from_directory(\n",
    "        back_test_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"\\nStatus of highlighted images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "high_train_gen = data_generator.flow_from_directory(\n",
    "        high_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"[Testing set]\")\n",
    "\n",
    "high_test_gen = data_generator.flow_from_directory(\n",
    "        high_test_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "# 設定 Callback 函式並在訓練完成時，計算測試資料集的 F1-score\n",
    "test_f1s = []\n",
    "\n",
    "class MetricsFront(Callback):\n",
    "\n",
    "    def __init__(self, test_data = front_test_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        batches = len(self.test_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        test_pred = np.zeros((total, 250))\n",
    "        test_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xTest, yTest = next(self.test_data)\n",
    "            test_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xTest)).round()\n",
    "            test_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yTest\n",
    "            \n",
    "        test_pred = np.squeeze(test_pred)\n",
    "        _test_f1 = f1_score(test_true, test_pred, average = 'micro')\n",
    "        test_f1s.append(_test_f1)\n",
    "        \n",
    "        return\n",
    "    \n",
    "class MetricsBack(Callback):\n",
    "\n",
    "    def __init__(self, test_data = back_test_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        batches = len(self.test_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        test_pred = np.zeros((total, 250))\n",
    "        test_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xTest, yTest = next(self.test_data)\n",
    "            test_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xTest)).round()\n",
    "            test_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yTest\n",
    "            \n",
    "        test_pred = np.squeeze(test_pred)\n",
    "        _test_f1 = f1_score(test_true, test_pred, average = 'micro')\n",
    "        test_f1s.append(_test_f1)\n",
    "        \n",
    "        return\n",
    "    \n",
    "class MetricsHigh(Callback):\n",
    "\n",
    "    def __init__(self, test_data = high_test_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        batches = len(self.test_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        test_pred = np.zeros((total, 250))\n",
    "        test_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xTest, yTest = next(self.test_data)\n",
    "            test_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xTest)).round()\n",
    "            test_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yTest\n",
    "            \n",
    "        test_pred = np.squeeze(test_pred)\n",
    "        _test_f1 = f1_score(test_true, test_pred, average = 'micro')\n",
    "        test_f1s.append(_test_f1)\n",
    "\n",
    "        return\n",
    "    \n",
    "# 重新訓練深度學習網路\n",
    "print(\"\\nFront images training:\")\n",
    "\n",
    "model_front.fit_generator(\n",
    "    front_train_gen,\n",
    "    steps_per_epoch = 375,\n",
    "    epochs = best_front_epochs,\n",
    "    callbacks = [MetricsFront()])\n",
    "\n",
    "print(\"\\nBack images training:\")\n",
    "\n",
    "model_back.fit_generator(\n",
    "    back_train_gen,\n",
    "    steps_per_epoch = 375,\n",
    "    epochs = best_back_epochs,\n",
    "    callbacks = [MetricsBack()])\n",
    "\n",
    "print(\"\\nHighlighted images training:\")\n",
    "\n",
    "model_high.fit_generator(\n",
    "    high_train_gen,\n",
    "    steps_per_epoch = 375,\n",
    "    epochs = best_high_epochs,\n",
    "    callbacks = [MetricsHigh()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實驗總結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TESTING RESULT ---\n",
      "[Front]\t\tThe f1-score of front testing set: 67.43%\n",
      "[Back]\t\tThe f1-score of back testing set: 50.73%\n",
      "[Highlighted]\tThe f1-score of highlighted testing set: 98.53%\n"
     ]
    }
   ],
   "source": [
    "print(\"--- TESTING RESULT ---\")\n",
    "print(\"[Front]\\t\\tThe f1-score of front testing set: %.2f%%\"%(test_f1s[0] * 100))\n",
    "print(\"[Back]\\t\\tThe f1-score of back testing set: %.2f%%\"%(test_f1s[1] * 100))\n",
    "print(\"[Highlighted]\\tThe f1-score of highlighted testing set: %.2f%%\"%(test_f1s[2] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
