{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誘導式深度學習 (Highlighted DL) 效果在 TensorFlow + Keras 上的實證"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實驗背景\n",
    "　　這是基於我的碩士論文「以誘導式深度學習為基礎之配藥核實技術及應用」所延伸的實驗（多國發明專利申請中，論文未公開）。在原始論文裡，我採用了 YOLO v2、ResNet 與 SE-ResNet 三種網路，證明「誘導式深度學習」能有效解決**訓練樣本稀少**、**藥物外觀相似**與**藥物種類繁多**等藥物識別上的難題，不但將藥物辨識率從 86% 提升至 99.8%，更採用 NVIDIA® Jetson TX2 嵌入式系統與模型結構優化技術，實現 6.23 幀 / 秒的實時藥物識別產品雛型。\n",
    "\n",
    "　　由於 YOLO v2、ResNet 與 SE-ResNet 皆屬較龐大的網路，所以，為了證明「誘導式深度學習」在簡單的 CNN 網路上**也能有卓越的識別率優化效果**，我將使用論文中未出現的 TensorFlow 與 Keras 框架重新進行實驗，也就是接下來這份文件中的內容。\n",
    "\n",
    "\n",
    "### 數據說明\n",
    "　　這項實驗所採用的數據與原始論文相同：成人錠劑**排藥包裝 (Blister Package)** 共 250 類，每個類別正、反面各 72 張，總計 36,000 張影像。對了！這是一份公開數據集，您可以在 [Google Drive](https://drive.google.com/folderview?id=1cV7JVYGRxcm9DY0o7BMJHTCRdo_RW17n) 或 [Baidu Pan](https://pan.baidu.com/s/1amfARVIhGIfYVIIfB9qFRg) 下載，供學術研究與非商業性質的實驗使用。\n",
    "\n",
    "\n",
    "### 實驗說明\n",
    "　　與原始論文相同，我將使用排藥的 **1. 正面原始影像**、**2. 背面原始影像**，與經過「誘導式深度學習」技術生成的 **3. 雙面拼整影像**資料集，對相同的 CNN 網路進行訓練與測試，目的在於觀察「誘導式深度學習」技術的採用與否，對排藥識別效果的影響會有多大。\n",
    "  \n",
    "　　在訓練樣本、測試樣本的分配上，將依照 3:1 的比例，對每類別 72 張圖像**隨機抽取** 54 張作為訓練集 (Training-set)、18 張作為測試集 (Testing-set)，總計訓練樣本 13,500 張，訓練樣本外 (Out-of-sample) 的測試樣本 4,500 張。訓練時，還會從每類別 54 張訓練圖像中再**隨機抽取** 18 張作為驗證集 (Validation-set)。\n",
    "  \n",
    "　　介紹完了，我們開始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 環境參數設定\n",
    "設置載入的套件與資料集路徑。在本實驗中，我將採用 150 x 150 的圖像輸入尺寸進行模型的訓練與測試，而非原始論文的 224 x 224。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image_size = 150\n",
    "\n",
    "# 正面原始影像的訓練與測試圖像路徑。\n",
    "front_train_dir = \"/home/ubuntu/Desktop/High/OriginalFrontTrain\"\n",
    "front_test_dir = \"/home/ubuntu/Desktop/High/OriginalFrontTest\"\n",
    "\n",
    "# 背面原始影像的訓練與測試圖像路徑。\n",
    "back_train_dir = \"/home/ubuntu/Desktop/High/OriginalBackTrain\"\n",
    "back_test_dir = \"/home/ubuntu/Desktop/High/OriginalBackTest\"\n",
    "\n",
    "# 雙面拼整影像的訓練與測試圖像路徑。\n",
    "high_train_dir = \"/home/ubuntu/Desktop/High/HighlightedTrain\"\n",
    "high_test_dir = \"/home/ubuntu/Desktop/High/HighlightedTest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 建立簡單的 CNN 模型（三種資料集都使用相同的模型結構）\n",
    "卷積神經網路中採用 Dropout 防止數據過擬合；最後一個全連接層使用 Softmax 作為激活函數，對 250 種排藥類別進行預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 250)               32250     \n",
      "=================================================================\n",
      "Total params: 494,298\n",
      "Trainable params: 494,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 訓練與識別正面原始影像的模型\n",
    "model_front = models.Sequential()\n",
    "model_front.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "\n",
    "model_front.add(layers.Flatten())\n",
    "model_front.add(layers.Dense(128, activation = 'relu'))\n",
    "model_front.add(Dropout(0.5))\n",
    "model_front.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "\n",
    "# 訓練與識別背面原始影像的模型\n",
    "model_back = models.Sequential()\n",
    "model_back.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "\n",
    "model_back.add(layers.Flatten())\n",
    "model_back.add(layers.Dense(128, activation = 'relu'))\n",
    "model_back.add(Dropout(0.5))\n",
    "model_back.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "\n",
    "# 訓練與識別雙面拼整影像的模型\n",
    "model_high = models.Sequential()\n",
    "model_high.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "\n",
    "model_high.add(layers.Flatten())\n",
    "model_high.add(layers.Dense(128, activation = 'relu'))\n",
    "model_high.add(Dropout(0.5))\n",
    "model_high.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "\n",
    "# 看看模型長什麼樣子\n",
    "model_high.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 編譯模型\n",
    "使用 'categorical_crossentropy' 作為損失函數，讓「網路輸出」的機率分佈與「真實標籤」的分佈盡可能一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_front.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_back.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_high.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 採用 Generator 批量生成訓練與驗證資料\n",
    "使用驗證資料集來確認該在何時執行 Early Stopping，以便稍後（Step 8）選用最佳模型進行測試資料集的效果評估。\n",
    "\n",
    "資料總計：訓練樣本 9,000 張影像、驗證樣本 4,500 張影像，共 250 類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of front images:\n",
      "[Training set]\n",
      "Found 9000 images belonging to 250 classes.\n",
      "[Validation set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Status of back images:\n",
      "[Training set]\n",
      "Found 9000 images belonging to 250 classes.\n",
      "[Validation set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Status of highlighted images:\n",
      "[Training set]\n",
      "Found 9000 images belonging to 250 classes.\n",
      "[Validation set]\n",
      "Found 4500 images belonging to 250 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 從同一個 training 資料路徑下拆分出 training 與 validation 的數據集。\n",
    "# 設定 validation_split=0.35，從每類 54 張訓練樣本中抽取 18 張作為驗證樣本。\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.35)\n",
    "\n",
    "# 正面原始影像的 Generator\n",
    "\n",
    "print(\"Status of front images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "front_train_gen = data_generator.flow_from_directory(\n",
    "        front_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical', # 多類別，所以使用 categorical 標籤，返回 2D 的 one-hot 編碼標籤。\n",
    "        batch_size = 36,\n",
    "        subset = \"training\") # 因為啟用了 validation_split，故可設定 train_generator 放的是「訓練子集」。\n",
    "\n",
    "print(\"[Validation set]\")\n",
    "\n",
    "front_validation_gen = data_generator.flow_from_directory(\n",
    "        front_train_dir, # 一樣從 training 的路徑下抓取驗證用數據。\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"validation\") # 因為啟用了 validation_split，故可設定 validation_generator 放的是「驗證子集」。\n",
    "\n",
    "# 背面原始影像的 Generator\n",
    "\n",
    "print(\"\\nStatus of back images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "back_train_gen = data_generator.flow_from_directory(\n",
    "        back_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"training\")\n",
    "\n",
    "print(\"[Validation set]\")\n",
    "\n",
    "back_validation_gen = data_generator.flow_from_directory(\n",
    "        back_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"validation\")\n",
    "\n",
    "# 雙面拼整影像的 Generator\n",
    "\n",
    "print(\"\\nStatus of highlighted images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "high_train_gen = data_generator.flow_from_directory(\n",
    "        high_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"training\")\n",
    "\n",
    "print(\"[Validation set]\")\n",
    "\n",
    "high_validation_gen = data_generator.flow_from_directory(\n",
    "        high_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36,\n",
    "        subset = \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 定義 Callback Function 以計算 Percision / Recall / F1-Score\n",
    "同原始論文，我們將以 Precision、Recall 與 F1-Score 作為實驗的評估標準。\n",
    "\n",
    "然而，我們不能使用 Keras 內建的矩陣函式（Metric function）進行 F1-Score 的計算！因為矩陣函式是根據 Batch Step 進行呼叫的，其所計算的 F1-score 將只是該 Batch 的平均結果，而不是每個 Epoch 的平均結果。\n",
    "\n",
    "此外，若要透過 Data Generator 來計算 F1-Score，我們必須自行設計對應的 Callback 函式。\n",
    "\n",
    "### 參考資料\n",
    "* https://medium.com/@thongonary/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2\n",
    "* https://github.com/keras-team/keras/issues/10472#issuecomment-472543538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "# 紀錄每個 Epoch 訓練結束時，使用驗證集測試得到的 F1-score 結果\n",
    "front_f1s = []\n",
    "back_f1s = []\n",
    "high_f1s = []\n",
    "\n",
    "class MetricsFront(Callback):\n",
    "\n",
    "    def __init__(self, val_data = front_validation_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        batches = len(self.validation_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        val_pred = np.zeros((total, 250))\n",
    "        val_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xVal, yVal = next(self.validation_data)\n",
    "            val_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xVal)).round()\n",
    "            val_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yVal\n",
    "            \n",
    "        val_pred = np.squeeze(val_pred)\n",
    "        _val_f1 = f1_score(val_true, val_pred, average = 'micro')\n",
    "        _val_precision = precision_score(val_true, val_pred, average = 'micro')\n",
    "        _val_recall = recall_score(val_true, val_pred, average = 'micro')\n",
    "        \n",
    "        front_f1s.append(_val_f1)\n",
    "\n",
    "        print(\"- val_f1: %f - val_precision: %f - val_recall: %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "    \n",
    "class MetricsBack(Callback):\n",
    "\n",
    "    def __init__(self, val_data = back_validation_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        batches = len(self.validation_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        val_pred = np.zeros((total, 250))\n",
    "        val_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xVal, yVal = next(self.validation_data)\n",
    "            val_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xVal)).round()\n",
    "            val_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yVal\n",
    "            \n",
    "        val_pred = np.squeeze(val_pred)\n",
    "        _val_f1 = f1_score(val_true, val_pred, average = 'micro')\n",
    "        _val_precision = precision_score(val_true, val_pred, average = 'micro')\n",
    "        _val_recall = recall_score(val_true, val_pred, average = 'micro')\n",
    "        \n",
    "        back_f1s.append(_val_f1)\n",
    "        \n",
    "        print(\"- val_f1: %f - val_precision: %f - val_recall: %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "    \n",
    "class MetricsHigh(Callback):\n",
    "\n",
    "    def __init__(self, val_data = high_validation_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        batches = len(self.validation_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        val_pred = np.zeros((total, 250))\n",
    "        val_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xVal, yVal = next(self.validation_data)\n",
    "            val_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xVal)).round()\n",
    "            val_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yVal\n",
    "            \n",
    "        val_pred = np.squeeze(val_pred)\n",
    "        _val_f1 = f1_score(val_true, val_pred, average = 'micro')\n",
    "        _val_precision = precision_score(val_true, val_pred, average = 'micro')\n",
    "        _val_recall = recall_score(val_true, val_pred, average = 'micro')\n",
    "        \n",
    "        high_f1s.append(_val_f1)\n",
    "        \n",
    "        print(\"- val_f1: %f - val_precision: %f - val_recall: %f\"%(_val_f1, _val_precision, _val_recall))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. 訓練與驗證深度學習網路\n",
    "此部分定義了訓練的資料路徑、訓練與驗證集的相關參數，以及要呼叫的 Callback Function。\n",
    "\n",
    "與原始論文相同，我將以 100 次的 Epoch 進行訓練，並透過驗證集來尋找 Early Stopping 的時機。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front images training:\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 141s 562ms/step - loss: 5.4337 - acc: 0.0054 - val_loss: 5.2365 - val_acc: 0.0113\n",
      "- val_f1: 0.000000 - val_precision: 0.000000 - val_recall: 0.000000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 5.0433 - acc: 0.0196 - val_loss: 4.6882 - val_acc: 0.0447\n",
      "- val_f1: 0.000000 - val_precision: 0.000000 - val_recall: 0.000000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 4.5510 - acc: 0.0473 - val_loss: 4.2991 - val_acc: 0.0904\n",
      "- val_f1: 0.007086 - val_precision: 1.000000 - val_recall: 0.003556\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 4.0916 - acc: 0.0831 - val_loss: 3.6843 - val_acc: 0.2018\n",
      "- val_f1: 0.027181 - val_precision: 1.000000 - val_recall: 0.013778\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 3.7069 - acc: 0.1339 - val_loss: 3.2694 - val_acc: 0.2584\n",
      "- val_f1: 0.061479 - val_precision: 0.940789 - val_recall: 0.031778\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 124s 494ms/step - loss: 3.3811 - acc: 0.1782 - val_loss: 3.0195 - val_acc: 0.3033\n",
      "- val_f1: 0.105705 - val_precision: 0.940299 - val_recall: 0.056000\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 3.1022 - acc: 0.2269 - val_loss: 2.6642 - val_acc: 0.3544\n",
      "- val_f1: 0.176411 - val_precision: 0.920335 - val_recall: 0.097556\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 2.8381 - acc: 0.2678 - val_loss: 2.8226 - val_acc: 0.3029\n",
      "- val_f1: 0.159839 - val_precision: 0.870330 - val_recall: 0.088000\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 2.6447 - acc: 0.3043 - val_loss: 2.1677 - val_acc: 0.4427\n",
      "- val_f1: 0.305718 - val_precision: 0.872385 - val_recall: 0.185333\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 123s 490ms/step - loss: 2.4718 - acc: 0.3419 - val_loss: 2.2030 - val_acc: 0.4376\n",
      "- val_f1: 0.323412 - val_precision: 0.882178 - val_recall: 0.198000\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 2.3003 - acc: 0.3793 - val_loss: 1.9249 - val_acc: 0.5096\n",
      "- val_f1: 0.352246 - val_precision: 0.905244 - val_recall: 0.218667\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 2.1702 - acc: 0.4059 - val_loss: 2.0620 - val_acc: 0.4333\n",
      "- val_f1: 0.357906 - val_precision: 0.843931 - val_recall: 0.227111\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 2.0484 - acc: 0.4312 - val_loss: 1.8169 - val_acc: 0.5093\n",
      "- val_f1: 0.425215 - val_precision: 0.887791 - val_recall: 0.279556\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.9694 - acc: 0.4454 - val_loss: 1.8109 - val_acc: 0.5060\n",
      "- val_f1: 0.385290 - val_precision: 0.849886 - val_recall: 0.249111\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.8644 - acc: 0.4699 - val_loss: 1.6089 - val_acc: 0.5596\n",
      "- val_f1: 0.470665 - val_precision: 0.882641 - val_recall: 0.320889\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.8056 - acc: 0.4803 - val_loss: 1.6105 - val_acc: 0.5556\n",
      "- val_f1: 0.480832 - val_precision: 0.893720 - val_recall: 0.328889\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 123s 490ms/step - loss: 1.7460 - acc: 0.5063 - val_loss: 1.4666 - val_acc: 0.5958\n",
      "- val_f1: 0.522921 - val_precision: 0.905805 - val_recall: 0.367556\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.6698 - acc: 0.5170 - val_loss: 1.3693 - val_acc: 0.6153\n",
      "- val_f1: 0.566089 - val_precision: 0.879625 - val_recall: 0.417333\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 123s 490ms/step - loss: 1.6284 - acc: 0.5337 - val_loss: 1.4978 - val_acc: 0.5831\n",
      "- val_f1: 0.523256 - val_precision: 0.893240 - val_recall: 0.370000\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.5936 - acc: 0.5421 - val_loss: 1.3621 - val_acc: 0.6120\n",
      "- val_f1: 0.560968 - val_precision: 0.902860 - val_recall: 0.406889\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.5620 - acc: 0.5463 - val_loss: 1.8639 - val_acc: 0.4896\n",
      "- val_f1: 0.465736 - val_precision: 0.864897 - val_recall: 0.318667\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 124s 496ms/step - loss: 1.4975 - acc: 0.5649 - val_loss: 1.7646 - val_acc: 0.5144\n",
      "- val_f1: 0.487168 - val_precision: 0.811820 - val_recall: 0.348000\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.4643 - acc: 0.5796 - val_loss: 1.2610 - val_acc: 0.6309\n",
      "- val_f1: 0.592984 - val_precision: 0.873325 - val_recall: 0.448889\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.4248 - acc: 0.5827 - val_loss: 1.4322 - val_acc: 0.5889\n",
      "- val_f1: 0.577320 - val_precision: 0.832845 - val_recall: 0.441778\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.4231 - acc: 0.5857 - val_loss: 1.3723 - val_acc: 0.6049\n",
      "- val_f1: 0.570274 - val_precision: 0.873453 - val_recall: 0.423333\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.3822 - acc: 0.5982 - val_loss: 1.3407 - val_acc: 0.6149\n",
      "- val_f1: 0.606793 - val_precision: 0.841545 - val_recall: 0.474444\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.3296 - acc: 0.6094 - val_loss: 1.2067 - val_acc: 0.6569\n",
      "- val_f1: 0.639178 - val_precision: 0.851646 - val_recall: 0.511556\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.3214 - acc: 0.6150 - val_loss: 1.1631 - val_acc: 0.6713\n",
      "- val_f1: 0.623791 - val_precision: 0.867089 - val_recall: 0.487111\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.3145 - acc: 0.6092 - val_loss: 1.2359 - val_acc: 0.6458\n",
      "- val_f1: 0.607287 - val_precision: 0.869205 - val_recall: 0.466667\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.3003 - acc: 0.6150 - val_loss: 1.0346 - val_acc: 0.6920\n",
      "- val_f1: 0.663760 - val_precision: 0.857746 - val_recall: 0.541333\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.2697 - acc: 0.6244 - val_loss: 1.3863 - val_acc: 0.5922\n",
      "- val_f1: 0.591854 - val_precision: 0.785872 - val_recall: 0.474667\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.2913 - acc: 0.6242 - val_loss: 1.0826 - val_acc: 0.6796\n",
      "- val_f1: 0.683883 - val_precision: 0.846280 - val_recall: 0.573778\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.2513 - acc: 0.6393 - val_loss: 1.2259 - val_acc: 0.6396\n",
      "- val_f1: 0.617045 - val_precision: 0.835611 - val_recall: 0.489111\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 123s 494ms/step - loss: 1.2545 - acc: 0.6351 - val_loss: 1.4036 - val_acc: 0.6102\n",
      "- val_f1: 0.609136 - val_precision: 0.791963 - val_recall: 0.494889\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.2220 - acc: 0.6433 - val_loss: 1.6288 - val_acc: 0.5538\n",
      "- val_f1: 0.576902 - val_precision: 0.777903 - val_recall: 0.458444\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.2220 - acc: 0.6443 - val_loss: 1.3788 - val_acc: 0.6067\n",
      "- val_f1: 0.598293 - val_precision: 0.786179 - val_recall: 0.482889\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.1904 - acc: 0.6573 - val_loss: 1.3131 - val_acc: 0.6309\n",
      "- val_f1: 0.630348 - val_precision: 0.783873 - val_recall: 0.527111\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 124s 494ms/step - loss: 1.1950 - acc: 0.6571 - val_loss: 1.4014 - val_acc: 0.5971\n",
      "- val_f1: 0.548492 - val_precision: 0.819346 - val_recall: 0.412222\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 124s 494ms/step - loss: 1.1908 - acc: 0.6530 - val_loss: 1.1091 - val_acc: 0.6840\n",
      "- val_f1: 0.648089 - val_precision: 0.854493 - val_recall: 0.522000\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 123s 494ms/step - loss: 1.1863 - acc: 0.6571 - val_loss: 1.4745 - val_acc: 0.5811\n",
      "- val_f1: 0.569620 - val_precision: 0.807504 - val_recall: 0.440000\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 124s 496ms/step - loss: 1.1685 - acc: 0.6612 - val_loss: 1.2499 - val_acc: 0.6402\n",
      "- val_f1: 0.605237 - val_precision: 0.833528 - val_recall: 0.475111\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.1648 - acc: 0.6639 - val_loss: 1.0784 - val_acc: 0.6998\n",
      "- val_f1: 0.657429 - val_precision: 0.859089 - val_recall: 0.532444\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.1616 - acc: 0.6654 - val_loss: 1.5219 - val_acc: 0.5969\n",
      "- val_f1: 0.607872 - val_precision: 0.776702 - val_recall: 0.499333\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.1753 - acc: 0.6621 - val_loss: 1.3524 - val_acc: 0.6167\n",
      "- val_f1: 0.612332 - val_precision: 0.764041 - val_recall: 0.510889\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.1864 - acc: 0.6596 - val_loss: 1.8589 - val_acc: 0.4893\n",
      "- val_f1: 0.460401 - val_precision: 0.750754 - val_recall: 0.332000\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 124s 495ms/step - loss: 1.1681 - acc: 0.6683 - val_loss: 1.2494 - val_acc: 0.6549\n",
      "- val_f1: 0.642973 - val_precision: 0.820345 - val_recall: 0.528667\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.1827 - acc: 0.6618 - val_loss: 1.4296 - val_acc: 0.5847\n",
      "- val_f1: 0.548258 - val_precision: 0.823608 - val_recall: 0.410889\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 124s 494ms/step - loss: 1.1650 - acc: 0.6650 - val_loss: 1.5588 - val_acc: 0.5938\n",
      "- val_f1: 0.586823 - val_precision: 0.733823 - val_recall: 0.488889\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 123s 494ms/step - loss: 1.1717 - acc: 0.6689 - val_loss: 1.4256 - val_acc: 0.6162\n",
      "- val_f1: 0.576224 - val_precision: 0.766704 - val_recall: 0.461556\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.1820 - acc: 0.6676 - val_loss: 1.2979 - val_acc: 0.6382\n",
      "- val_f1: 0.593824 - val_precision: 0.787946 - val_recall: 0.476444\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.1630 - acc: 0.6709 - val_loss: 1.3088 - val_acc: 0.6356\n",
      "- val_f1: 0.587539 - val_precision: 0.803392 - val_recall: 0.463111\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.1833 - acc: 0.6706 - val_loss: 1.5313 - val_acc: 0.5953\n",
      "- val_f1: 0.585593 - val_precision: 0.728505 - val_recall: 0.489556\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.1991 - acc: 0.6714 - val_loss: 1.6367 - val_acc: 0.5880\n",
      "- val_f1: 0.587559 - val_precision: 0.713198 - val_recall: 0.499556\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.1688 - acc: 0.6748 - val_loss: 2.2719 - val_acc: 0.4251\n",
      "- val_f1: 0.390361 - val_precision: 0.704348 - val_recall: 0.270000\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.1424 - acc: 0.6809 - val_loss: 1.0336 - val_acc: 0.7091\n",
      "- val_f1: 0.683883 - val_precision: 0.846280 - val_recall: 0.573778\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.1615 - acc: 0.6806 - val_loss: 1.4213 - val_acc: 0.6133\n",
      "- val_f1: 0.590615 - val_precision: 0.802903 - val_recall: 0.467111\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 123s 490ms/step - loss: 1.1764 - acc: 0.6713 - val_loss: 1.3250 - val_acc: 0.6476\n",
      "- val_f1: 0.641678 - val_precision: 0.782109 - val_recall: 0.544000\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.1996 - acc: 0.6709 - val_loss: 1.6035 - val_acc: 0.5667\n",
      "- val_f1: 0.493622 - val_precision: 0.742194 - val_recall: 0.369778\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.1744 - acc: 0.6767 - val_loss: 1.4042 - val_acc: 0.6164\n",
      "- val_f1: 0.580645 - val_precision: 0.805282 - val_recall: 0.454000\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.1870 - acc: 0.6773 - val_loss: 1.4499 - val_acc: 0.6031\n",
      "- val_f1: 0.565712 - val_precision: 0.737934 - val_recall: 0.458667\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.1806 - acc: 0.6766 - val_loss: 1.5780 - val_acc: 0.5927\n",
      "- val_f1: 0.555954 - val_precision: 0.731858 - val_recall: 0.448222\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.2195 - acc: 0.6709 - val_loss: 1.5145 - val_acc: 0.5947\n",
      "- val_f1: 0.559079 - val_precision: 0.758568 - val_recall: 0.442667\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.1894 - acc: 0.6718 - val_loss: 1.7036 - val_acc: 0.5520\n",
      "- val_f1: 0.491957 - val_precision: 0.719418 - val_recall: 0.373778\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.1811 - acc: 0.6807 - val_loss: 1.7534 - val_acc: 0.5680\n",
      "- val_f1: 0.561955 - val_precision: 0.692834 - val_recall: 0.472667\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.2092 - acc: 0.6726 - val_loss: 1.9840 - val_acc: 0.5269\n",
      "- val_f1: 0.511794 - val_precision: 0.668338 - val_recall: 0.414667\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.1972 - acc: 0.6758 - val_loss: 1.3193 - val_acc: 0.6553\n",
      "- val_f1: 0.643269 - val_precision: 0.783163 - val_recall: 0.545778\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.2010 - acc: 0.6746 - val_loss: 1.4741 - val_acc: 0.5967\n",
      "- val_f1: 0.558020 - val_precision: 0.783447 - val_recall: 0.433333\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.2001 - acc: 0.6771 - val_loss: 2.6656 - val_acc: 0.3913\n",
      "- val_f1: 0.347326 - val_precision: 0.578157 - val_recall: 0.248222\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.2195 - acc: 0.6697 - val_loss: 1.7999 - val_acc: 0.5429\n",
      "- val_f1: 0.503783 - val_precision: 0.729764 - val_recall: 0.384667\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 123s 490ms/step - loss: 1.2335 - acc: 0.6690 - val_loss: 1.3832 - val_acc: 0.6260\n",
      "- val_f1: 0.609415 - val_precision: 0.782306 - val_recall: 0.499111\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.2110 - acc: 0.6787 - val_loss: 1.6935 - val_acc: 0.5516\n",
      "- val_f1: 0.503557 - val_precision: 0.726435 - val_recall: 0.385333\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 122s 488ms/step - loss: 1.2122 - acc: 0.6707 - val_loss: 1.6907 - val_acc: 0.5489\n",
      "- val_f1: 0.492373 - val_precision: 0.768505 - val_recall: 0.362222\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 122s 489ms/step - loss: 1.2136 - acc: 0.6727 - val_loss: 2.0756 - val_acc: 0.4782\n",
      "- val_f1: 0.432161 - val_precision: 0.686502 - val_recall: 0.315333\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 123s 493ms/step - loss: 1.2161 - acc: 0.6766 - val_loss: 1.8961 - val_acc: 0.5176\n",
      "- val_f1: 0.481810 - val_precision: 0.677679 - val_recall: 0.373778\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.2116 - acc: 0.6751 - val_loss: 1.8365 - val_acc: 0.5440\n",
      "- val_f1: 0.524194 - val_precision: 0.663265 - val_recall: 0.433333\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.2474 - acc: 0.6678 - val_loss: 1.6213 - val_acc: 0.5804\n",
      "- val_f1: 0.532610 - val_precision: 0.744316 - val_recall: 0.414667\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.2283 - acc: 0.6760 - val_loss: 2.1577 - val_acc: 0.4702\n",
      "- val_f1: 0.458136 - val_precision: 0.647584 - val_recall: 0.354444\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 122s 490ms/step - loss: 1.2246 - acc: 0.6750 - val_loss: 1.3017 - val_acc: 0.6298\n",
      "- val_f1: 0.601920 - val_precision: 0.825077 - val_recall: 0.473778\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 1.2564 - acc: 0.6681 - val_loss: 1.8819 - val_acc: 0.5009\n",
      "- val_f1: 0.449414 - val_precision: 0.758311 - val_recall: 0.319333\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 123s 492ms/step - loss: 1.2402 - acc: 0.6689 - val_loss: 1.5882 - val_acc: 0.5802\n",
      "- val_f1: 0.542155 - val_precision: 0.794168 - val_recall: 0.411556\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 122s 490ms/step - loss: 1.2487 - acc: 0.6707 - val_loss: 1.7525 - val_acc: 0.5498\n",
      "- val_f1: 0.534749 - val_precision: 0.704578 - val_recall: 0.430889\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 124s 495ms/step - loss: 1.2358 - acc: 0.6744 - val_loss: 2.0620 - val_acc: 0.5084\n",
      "- val_f1: 0.479102 - val_precision: 0.649430 - val_recall: 0.379556\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 125s 498ms/step - loss: 1.2572 - acc: 0.6740 - val_loss: 2.0521 - val_acc: 0.4804\n",
      "- val_f1: 0.430168 - val_precision: 0.635534 - val_recall: 0.325111\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 124s 498ms/step - loss: 1.2761 - acc: 0.6661 - val_loss: 1.7121 - val_acc: 0.5660\n",
      "- val_f1: 0.553920 - val_precision: 0.750665 - val_recall: 0.438889\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 124s 495ms/step - loss: 1.2697 - acc: 0.6612 - val_loss: 1.7921 - val_acc: 0.5329\n",
      "- val_f1: 0.507050 - val_precision: 0.693673 - val_recall: 0.399556\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 125s 500ms/step - loss: 1.2475 - acc: 0.6729 - val_loss: 2.1360 - val_acc: 0.4742\n",
      "- val_f1: 0.444476 - val_precision: 0.631600 - val_recall: 0.342889\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 124s 496ms/step - loss: 1.3053 - acc: 0.6621 - val_loss: 2.5098 - val_acc: 0.3980\n",
      "- val_f1: 0.352091 - val_precision: 0.571357 - val_recall: 0.254444\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 124s 498ms/step - loss: 1.2856 - acc: 0.6676 - val_loss: 2.4600 - val_acc: 0.3722\n",
      "- val_f1: 0.333114 - val_precision: 0.640329 - val_recall: 0.225111\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 125s 501ms/step - loss: 1.2776 - acc: 0.6641 - val_loss: 1.9075 - val_acc: 0.4998\n",
      "- val_f1: 0.447939 - val_precision: 0.715610 - val_recall: 0.326000\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 125s 502ms/step - loss: 1.2993 - acc: 0.6612 - val_loss: 2.5444 - val_acc: 0.4093\n",
      "- val_f1: 0.384007 - val_precision: 0.584203 - val_recall: 0.286000\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 126s 504ms/step - loss: 1.3076 - acc: 0.6634 - val_loss: 2.7380 - val_acc: 0.3440\n",
      "- val_f1: 0.303997 - val_precision: 0.585180 - val_recall: 0.205333\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 127s 508ms/step - loss: 1.2753 - acc: 0.6678 - val_loss: 1.9256 - val_acc: 0.5413\n",
      "- val_f1: 0.528067 - val_precision: 0.701657 - val_recall: 0.423333\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 1.3168 - acc: 0.6591 - val_loss: 2.5752 - val_acc: 0.4300\n",
      "- val_f1: 0.396116 - val_precision: 0.554135 - val_recall: 0.308222\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 1.3275 - acc: 0.6644 - val_loss: 2.3308 - val_acc: 0.4071\n",
      "- val_f1: 0.354102 - val_precision: 0.680354 - val_recall: 0.239333\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.3671 - acc: 0.6542 - val_loss: 1.9587 - val_acc: 0.4896\n",
      "- val_f1: 0.444992 - val_precision: 0.725628 - val_recall: 0.320889\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 1.3035 - acc: 0.6622 - val_loss: 2.9977 - val_acc: 0.4513\n",
      "- val_f1: 0.444003 - val_precision: 0.549116 - val_recall: 0.372667\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 1.3343 - acc: 0.6624 - val_loss: 1.8008 - val_acc: 0.5269\n",
      "- val_f1: 0.490803 - val_precision: 0.715319 - val_recall: 0.373556\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 128s 513ms/step - loss: 1.3199 - acc: 0.6587 - val_loss: 2.6686 - val_acc: 0.3978\n",
      "- val_f1: 0.352219 - val_precision: 0.569796 - val_recall: 0.254889\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 128s 512ms/step - loss: 1.3404 - acc: 0.6581 - val_loss: 2.1607 - val_acc: 0.4749\n",
      "- val_f1: 0.419538 - val_precision: 0.674545 - val_recall: 0.304444\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 128s 513ms/step - loss: 1.3527 - acc: 0.6596 - val_loss: 2.9408 - val_acc: 0.3738\n",
      "- val_f1: 0.354947 - val_precision: 0.563285 - val_recall: 0.259111\n",
      "\n",
      "Back images training:\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 148s 592ms/step - loss: 5.5265 - acc: 0.0037 - val_loss: 5.5215 - val_acc: 0.0040\n",
      "- val_f1: 0.000000 - val_precision: 0.000000 - val_recall: 0.000000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 134s 538ms/step - loss: 5.4939 - acc: 0.0059 - val_loss: 5.3967 - val_acc: 0.0076\n",
      "- val_f1: 0.000000 - val_precision: 0.000000 - val_recall: 0.000000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 136s 546ms/step - loss: 5.2513 - acc: 0.0109 - val_loss: 5.0232 - val_acc: 0.0242\n",
      "- val_f1: 0.000444 - val_precision: 0.500000 - val_recall: 0.000222\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 4.8645 - acc: 0.0311 - val_loss: 4.5474 - val_acc: 0.0662\n",
      "- val_f1: 0.008409 - val_precision: 1.000000 - val_recall: 0.004222\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 136s 544ms/step - loss: 4.4303 - acc: 0.0632 - val_loss: 4.4096 - val_acc: 0.0947\n",
      "- val_f1: 0.005757 - val_precision: 0.812500 - val_recall: 0.002889\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 136s 546ms/step - loss: 4.0297 - acc: 0.0987 - val_loss: 3.6747 - val_acc: 0.1771\n",
      "- val_f1: 0.027559 - val_precision: 0.875000 - val_recall: 0.014000\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 3.6594 - acc: 0.1402 - val_loss: 3.5653 - val_acc: 0.1827\n",
      "- val_f1: 0.042931 - val_precision: 0.883929 - val_recall: 0.022000\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 136s 544ms/step - loss: 3.3375 - acc: 0.1851 - val_loss: 3.1093 - val_acc: 0.2404\n",
      "- val_f1: 0.065427 - val_precision: 0.864407 - val_recall: 0.034000\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 3.0928 - acc: 0.2220 - val_loss: 2.7558 - val_acc: 0.3242\n",
      "- val_f1: 0.076401 - val_precision: 0.849057 - val_recall: 0.040000\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 2.8695 - acc: 0.2598 - val_loss: 2.5483 - val_acc: 0.3698\n",
      "- val_f1: 0.116424 - val_precision: 0.903226 - val_recall: 0.062222\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 2.6548 - acc: 0.2957 - val_loss: 2.1409 - val_acc: 0.4711\n",
      "- val_f1: 0.180259 - val_precision: 0.877670 - val_recall: 0.100444\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 2.4993 - acc: 0.3243 - val_loss: 2.4084 - val_acc: 0.3631\n",
      "- val_f1: 0.170475 - val_precision: 0.804878 - val_recall: 0.095333\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 2.3397 - acc: 0.3568 - val_loss: 2.6990 - val_acc: 0.3120\n",
      "- val_f1: 0.194545 - val_precision: 0.686406 - val_recall: 0.113333\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 135s 541ms/step - loss: 2.1968 - acc: 0.4030 - val_loss: 2.1496 - val_acc: 0.4218\n",
      "- val_f1: 0.213178 - val_precision: 0.833333 - val_recall: 0.122222\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 135s 541ms/step - loss: 2.0874 - acc: 0.4169 - val_loss: 1.7442 - val_acc: 0.5282\n",
      "- val_f1: 0.357218 - val_precision: 0.867925 - val_recall: 0.224889\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 1.9685 - acc: 0.4414 - val_loss: 1.5096 - val_acc: 0.6042\n",
      "- val_f1: 0.431207 - val_precision: 0.904187 - val_recall: 0.283111\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 136s 546ms/step - loss: 1.8585 - acc: 0.4698 - val_loss: 1.6813 - val_acc: 0.5300\n",
      "- val_f1: 0.410638 - val_precision: 0.799871 - val_recall: 0.276222\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 136s 545ms/step - loss: 1.7828 - acc: 0.4881 - val_loss: 2.0117 - val_acc: 0.4664\n",
      "- val_f1: 0.371131 - val_precision: 0.763850 - val_recall: 0.245111\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 135s 541ms/step - loss: 1.7002 - acc: 0.5106 - val_loss: 1.4948 - val_acc: 0.5802\n",
      "- val_f1: 0.496547 - val_precision: 0.845085 - val_recall: 0.351556\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 136s 546ms/step - loss: 1.6246 - acc: 0.5286 - val_loss: 1.5474 - val_acc: 0.5584\n",
      "- val_f1: 0.490160 - val_precision: 0.795409 - val_recall: 0.354222\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 134s 537ms/step - loss: 1.5740 - acc: 0.5492 - val_loss: 1.7070 - val_acc: 0.5249\n",
      "- val_f1: 0.482094 - val_precision: 0.774336 - val_recall: 0.350000\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 136s 544ms/step - loss: 1.4794 - acc: 0.5667 - val_loss: 1.6133 - val_acc: 0.5504\n",
      "- val_f1: 0.520218 - val_precision: 0.768796 - val_recall: 0.393111\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 136s 546ms/step - loss: 1.4255 - acc: 0.5858 - val_loss: 1.4098 - val_acc: 0.6024\n",
      "- val_f1: 0.575610 - val_precision: 0.812146 - val_recall: 0.445778\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 135s 542ms/step - loss: 1.3644 - acc: 0.5949 - val_loss: 1.6839 - val_acc: 0.5378\n",
      "- val_f1: 0.519841 - val_precision: 0.717527 - val_recall: 0.407556\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 137s 547ms/step - loss: 1.3245 - acc: 0.6053 - val_loss: 1.1799 - val_acc: 0.6656\n",
      "- val_f1: 0.643497 - val_precision: 0.807447 - val_recall: 0.534889\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 136s 543ms/step - loss: 1.3069 - acc: 0.6076 - val_loss: 1.3332 - val_acc: 0.6216\n",
      "- val_f1: 0.601527 - val_precision: 0.826983 - val_recall: 0.472667\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 135s 542ms/step - loss: 1.2362 - acc: 0.6270 - val_loss: 1.0783 - val_acc: 0.6798\n",
      "- val_f1: 0.664113 - val_precision: 0.818626 - val_recall: 0.558667\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 135s 542ms/step - loss: 1.2016 - acc: 0.6380 - val_loss: 1.3590 - val_acc: 0.6009\n",
      "- val_f1: 0.567454 - val_precision: 0.771789 - val_recall: 0.448667\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 135s 541ms/step - loss: 1.1865 - acc: 0.6470 - val_loss: 1.0822 - val_acc: 0.6836\n",
      "- val_f1: 0.652693 - val_precision: 0.809743 - val_recall: 0.546667\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 130s 521ms/step - loss: 1.1551 - acc: 0.6577 - val_loss: 1.5701 - val_acc: 0.5678\n",
      "- val_f1: 0.556698 - val_precision: 0.711765 - val_recall: 0.457111\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 1.1500 - acc: 0.6554 - val_loss: 1.3037 - val_acc: 0.6313\n",
      "- val_f1: 0.627986 - val_precision: 0.767158 - val_recall: 0.531556\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.1114 - acc: 0.6702 - val_loss: 1.3506 - val_acc: 0.6196\n",
      "- val_f1: 0.626367 - val_precision: 0.743511 - val_recall: 0.541111\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 128s 512ms/step - loss: 1.1006 - acc: 0.6708 - val_loss: 1.0912 - val_acc: 0.6876\n",
      "- val_f1: 0.668068 - val_precision: 0.817363 - val_recall: 0.564889\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 128s 513ms/step - loss: 1.0976 - acc: 0.6726 - val_loss: 0.9975 - val_acc: 0.7136\n",
      "- val_f1: 0.696551 - val_precision: 0.831842 - val_recall: 0.599111\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 128s 513ms/step - loss: 1.0412 - acc: 0.6857 - val_loss: 0.9796 - val_acc: 0.7080\n",
      "- val_f1: 0.698421 - val_precision: 0.826999 - val_recall: 0.604444\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 1.0435 - acc: 0.6894 - val_loss: 1.4364 - val_acc: 0.5980\n",
      "- val_f1: 0.593218 - val_precision: 0.747383 - val_recall: 0.491778\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.0317 - acc: 0.6990 - val_loss: 1.3144 - val_acc: 0.6251\n",
      "- val_f1: 0.626073 - val_precision: 0.755179 - val_recall: 0.534667\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9917 - acc: 0.7028 - val_loss: 1.2691 - val_acc: 0.6491\n",
      "- val_f1: 0.637745 - val_precision: 0.757956 - val_recall: 0.550444\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 130s 521ms/step - loss: 0.9878 - acc: 0.7103 - val_loss: 1.2022 - val_acc: 0.6540\n",
      "- val_f1: 0.656580 - val_precision: 0.767311 - val_recall: 0.573778\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9894 - acc: 0.7069 - val_loss: 0.9079 - val_acc: 0.7380\n",
      "- val_f1: 0.746631 - val_precision: 0.814008 - val_recall: 0.689556\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9911 - acc: 0.7042 - val_loss: 1.1403 - val_acc: 0.6909\n",
      "- val_f1: 0.687065 - val_precision: 0.780226 - val_recall: 0.613778\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9567 - acc: 0.7234 - val_loss: 0.9823 - val_acc: 0.7224\n",
      "- val_f1: 0.717439 - val_precision: 0.800493 - val_recall: 0.650000\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9579 - acc: 0.7210 - val_loss: 1.2170 - val_acc: 0.6849\n",
      "- val_f1: 0.664548 - val_precision: 0.776359 - val_recall: 0.580889\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 0.9464 - acc: 0.7264 - val_loss: 1.0994 - val_acc: 0.7087\n",
      "- val_f1: 0.706507 - val_precision: 0.794943 - val_recall: 0.635778\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.9663 - acc: 0.7159 - val_loss: 1.0564 - val_acc: 0.7036\n",
      "- val_f1: 0.689708 - val_precision: 0.817545 - val_recall: 0.596444\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9348 - acc: 0.7237 - val_loss: 1.3765 - val_acc: 0.6302\n",
      "- val_f1: 0.632203 - val_precision: 0.752840 - val_recall: 0.544889\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.9286 - acc: 0.7288 - val_loss: 1.0408 - val_acc: 0.7113\n",
      "- val_f1: 0.700998 - val_precision: 0.798580 - val_recall: 0.624667\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9499 - acc: 0.7263 - val_loss: 1.1947 - val_acc: 0.6809\n",
      "- val_f1: 0.665138 - val_precision: 0.779570 - val_recall: 0.580000\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9664 - acc: 0.7206 - val_loss: 1.3852 - val_acc: 0.6364\n",
      "- val_f1: 0.631141 - val_precision: 0.728621 - val_recall: 0.556667\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9278 - acc: 0.7311 - val_loss: 0.9683 - val_acc: 0.7453\n",
      "- val_f1: 0.742112 - val_precision: 0.821246 - val_recall: 0.676889\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.9393 - acc: 0.7312 - val_loss: 1.2010 - val_acc: 0.6907\n",
      "- val_f1: 0.692780 - val_precision: 0.774149 - val_recall: 0.626889\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 0.9166 - acc: 0.7374 - val_loss: 1.0744 - val_acc: 0.7133\n",
      "- val_f1: 0.717397 - val_precision: 0.787307 - val_recall: 0.658889\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9171 - acc: 0.7353 - val_loss: 1.2602 - val_acc: 0.6689\n",
      "- val_f1: 0.660286 - val_precision: 0.775480 - val_recall: 0.574889\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 0.9309 - acc: 0.7373 - val_loss: 1.0826 - val_acc: 0.7073\n",
      "- val_f1: 0.701608 - val_precision: 0.815240 - val_recall: 0.615778\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 0.9158 - acc: 0.7400 - val_loss: 1.0894 - val_acc: 0.7044\n",
      "- val_f1: 0.688979 - val_precision: 0.806135 - val_recall: 0.601556\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9221 - acc: 0.7431 - val_loss: 1.0990 - val_acc: 0.7136\n",
      "- val_f1: 0.691408 - val_precision: 0.807601 - val_recall: 0.604444\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 128s 514ms/step - loss: 0.9568 - acc: 0.7312 - val_loss: 1.0045 - val_acc: 0.7360\n",
      "- val_f1: 0.705563 - val_precision: 0.841700 - val_recall: 0.607333\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 128s 513ms/step - loss: 0.9432 - acc: 0.7369 - val_loss: 1.1800 - val_acc: 0.6798\n",
      "- val_f1: 0.664969 - val_precision: 0.778706 - val_recall: 0.580222\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9241 - acc: 0.7416 - val_loss: 1.2415 - val_acc: 0.6724\n",
      "- val_f1: 0.631737 - val_precision: 0.814958 - val_recall: 0.515778\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9184 - acc: 0.7448 - val_loss: 1.2480 - val_acc: 0.6718\n",
      "- val_f1: 0.651205 - val_precision: 0.780920 - val_recall: 0.558444\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9341 - acc: 0.7438 - val_loss: 1.1072 - val_acc: 0.6956\n",
      "- val_f1: 0.692863 - val_precision: 0.806433 - val_recall: 0.607333\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9521 - acc: 0.7432 - val_loss: 1.0371 - val_acc: 0.7291\n",
      "- val_f1: 0.716299 - val_precision: 0.816141 - val_recall: 0.638222\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9498 - acc: 0.7387 - val_loss: 1.4615 - val_acc: 0.6331\n",
      "- val_f1: 0.573636 - val_precision: 0.865859 - val_recall: 0.428889\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9360 - acc: 0.7464 - val_loss: 1.1235 - val_acc: 0.7009\n",
      "- val_f1: 0.678777 - val_precision: 0.799819 - val_recall: 0.589556\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9679 - acc: 0.7380 - val_loss: 2.0560 - val_acc: 0.5282\n",
      "- val_f1: 0.517836 - val_precision: 0.649246 - val_recall: 0.430667\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9440 - acc: 0.7416 - val_loss: 1.0580 - val_acc: 0.7216\n",
      "- val_f1: 0.703937 - val_precision: 0.814544 - val_recall: 0.619778\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9623 - acc: 0.7343 - val_loss: 1.3264 - val_acc: 0.6644\n",
      "- val_f1: 0.654093 - val_precision: 0.753258 - val_recall: 0.578000\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 130s 520ms/step - loss: 0.9587 - acc: 0.7458 - val_loss: 1.3909 - val_acc: 0.6498\n",
      "- val_f1: 0.637238 - val_precision: 0.739577 - val_recall: 0.559778\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9665 - acc: 0.7409 - val_loss: 1.0585 - val_acc: 0.7182\n",
      "- val_f1: 0.709597 - val_precision: 0.809280 - val_recall: 0.631778\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9556 - acc: 0.7433 - val_loss: 1.3666 - val_acc: 0.6804\n",
      "- val_f1: 0.661067 - val_precision: 0.773607 - val_recall: 0.577111\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9777 - acc: 0.7362 - val_loss: 1.0088 - val_acc: 0.7449\n",
      "- val_f1: 0.724795 - val_precision: 0.838540 - val_recall: 0.638222\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 0.9569 - acc: 0.7482 - val_loss: 1.4396 - val_acc: 0.6358\n",
      "- val_f1: 0.620833 - val_precision: 0.749686 - val_recall: 0.529778\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9687 - acc: 0.7463 - val_loss: 1.3694 - val_acc: 0.6567\n",
      "- val_f1: 0.644283 - val_precision: 0.773180 - val_recall: 0.552222\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 130s 519ms/step - loss: 0.9685 - acc: 0.7401 - val_loss: 1.8515 - val_acc: 0.6131\n",
      "- val_f1: 0.612205 - val_precision: 0.680718 - val_recall: 0.556222\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9823 - acc: 0.7426 - val_loss: 1.3636 - val_acc: 0.6511\n",
      "- val_f1: 0.646226 - val_precision: 0.777917 - val_recall: 0.552667\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.9667 - acc: 0.7469 - val_loss: 1.5647 - val_acc: 0.6147\n",
      "- val_f1: 0.602547 - val_precision: 0.747531 - val_recall: 0.504667\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9748 - acc: 0.7358 - val_loss: 0.9988 - val_acc: 0.7324\n",
      "- val_f1: 0.712169 - val_precision: 0.827789 - val_recall: 0.624889\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9770 - acc: 0.7450 - val_loss: 1.5749 - val_acc: 0.6020\n",
      "- val_f1: 0.592653 - val_precision: 0.755510 - val_recall: 0.487556\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.9547 - acc: 0.7457 - val_loss: 1.6205 - val_acc: 0.5911\n",
      "- val_f1: 0.575263 - val_precision: 0.762097 - val_recall: 0.462000\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 0.9611 - acc: 0.7441 - val_loss: 1.2815 - val_acc: 0.6724\n",
      "- val_f1: 0.608309 - val_precision: 0.811643 - val_recall: 0.486444\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 130s 518ms/step - loss: 1.0145 - acc: 0.7346 - val_loss: 1.7158 - val_acc: 0.6040\n",
      "- val_f1: 0.588485 - val_precision: 0.730086 - val_recall: 0.492889\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 129s 518ms/step - loss: 1.0173 - acc: 0.7382 - val_loss: 1.3139 - val_acc: 0.6740\n",
      "- val_f1: 0.629001 - val_precision: 0.818149 - val_recall: 0.510889\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 0.9790 - acc: 0.7431 - val_loss: 1.2984 - val_acc: 0.6762\n",
      "- val_f1: 0.674236 - val_precision: 0.768706 - val_recall: 0.600444\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 1.0223 - acc: 0.7316 - val_loss: 1.6368 - val_acc: 0.6276\n",
      "- val_f1: 0.604794 - val_precision: 0.761038 - val_recall: 0.501778\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.0528 - acc: 0.7313 - val_loss: 1.5461 - val_acc: 0.6289\n",
      "- val_f1: 0.634226 - val_precision: 0.735346 - val_recall: 0.557556\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.0084 - acc: 0.7392 - val_loss: 1.2151 - val_acc: 0.6862\n",
      "- val_f1: 0.666840 - val_precision: 0.804648 - val_recall: 0.569333\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.0206 - acc: 0.7386 - val_loss: 1.3523 - val_acc: 0.6549\n",
      "- val_f1: 0.643549 - val_precision: 0.782623 - val_recall: 0.546444\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.0466 - acc: 0.7321 - val_loss: 1.5137 - val_acc: 0.6164\n",
      "- val_f1: 0.589322 - val_precision: 0.740666 - val_recall: 0.489333\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 1.0336 - acc: 0.7372 - val_loss: 2.3163 - val_acc: 0.4727\n",
      "- val_f1: 0.454285 - val_precision: 0.639629 - val_recall: 0.352222\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 1.0329 - acc: 0.7348 - val_loss: 2.0996 - val_acc: 0.5296\n",
      "- val_f1: 0.523184 - val_precision: 0.659014 - val_recall: 0.433778\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 1.0298 - acc: 0.7373 - val_loss: 1.1925 - val_acc: 0.6882\n",
      "- val_f1: 0.676732 - val_precision: 0.807018 - val_recall: 0.582667\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 1.0239 - acc: 0.7256 - val_loss: 2.8823 - val_acc: 0.4773\n",
      "- val_f1: 0.477604 - val_precision: 0.550464 - val_recall: 0.421778\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 129s 517ms/step - loss: 1.0441 - acc: 0.7337 - val_loss: 1.1640 - val_acc: 0.6951\n",
      "- val_f1: 0.674736 - val_precision: 0.814326 - val_recall: 0.576000\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.0780 - acc: 0.7318 - val_loss: 1.0906 - val_acc: 0.7211\n",
      "- val_f1: 0.698550 - val_precision: 0.847764 - val_recall: 0.594000\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 1.0276 - acc: 0.7377 - val_loss: 1.3051 - val_acc: 0.6600\n",
      "- val_f1: 0.657239 - val_precision: 0.789589 - val_recall: 0.562889\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 1.0465 - acc: 0.7300 - val_loss: 1.7338 - val_acc: 0.6124\n",
      "- val_f1: 0.603032 - val_precision: 0.714677 - val_recall: 0.521556\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 1.0783 - acc: 0.7278 - val_loss: 2.1985 - val_acc: 0.5042\n",
      "- val_f1: 0.496767 - val_precision: 0.630643 - val_recall: 0.409778\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 129s 514ms/step - loss: 1.0844 - acc: 0.7284 - val_loss: 1.8854 - val_acc: 0.5447\n",
      "- val_f1: 0.519335 - val_precision: 0.698460 - val_recall: 0.413333\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.0586 - acc: 0.7327 - val_loss: 1.3387 - val_acc: 0.6584\n",
      "- val_f1: 0.607559 - val_precision: 0.826137 - val_recall: 0.480444\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 129s 515ms/step - loss: 1.0781 - acc: 0.7249 - val_loss: 1.4494 - val_acc: 0.6336\n",
      "- val_f1: 0.624626 - val_precision: 0.752742 - val_recall: 0.533778\n",
      "\n",
      "Highlighted images training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 110s 439ms/step - loss: 5.4928 - acc: 0.0068 - val_loss: 5.2928 - val_acc: 0.0356\n",
      "- val_f1: 0.003992 - val_precision: 1.000000 - val_recall: 0.002000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 4.8083 - acc: 0.0556 - val_loss: 3.9823 - val_acc: 0.1764\n",
      "- val_f1: 0.056107 - val_precision: 0.970149 - val_recall: 0.028889\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 3.7790 - acc: 0.1627 - val_loss: 2.8398 - val_acc: 0.4596\n",
      "- val_f1: 0.202582 - val_precision: 0.953271 - val_recall: 0.113333\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 2.8687 - acc: 0.3051 - val_loss: 1.7741 - val_acc: 0.6567\n",
      "- val_f1: 0.440364 - val_precision: 0.966843 - val_recall: 0.285111\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 2.2562 - acc: 0.4281 - val_loss: 1.3518 - val_acc: 0.7282\n",
      "- val_f1: 0.568865 - val_precision: 0.975175 - val_recall: 0.401556\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 1.8274 - acc: 0.5187 - val_loss: 0.9977 - val_acc: 0.8176\n",
      "- val_f1: 0.675543 - val_precision: 0.968465 - val_recall: 0.518667\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 1.4680 - acc: 0.5986 - val_loss: 0.5927 - val_acc: 0.8833\n",
      "- val_f1: 0.827985 - val_precision: 0.980243 - val_recall: 0.716667\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 1.2359 - acc: 0.6493 - val_loss: 0.6509 - val_acc: 0.8527\n",
      "- val_f1: 0.808462 - val_precision: 0.955455 - val_recall: 0.700667\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 1.0310 - acc: 0.7091 - val_loss: 0.4581 - val_acc: 0.8927\n",
      "- val_f1: 0.880422 - val_precision: 0.955278 - val_recall: 0.816444\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.8845 - acc: 0.7496 - val_loss: 0.3386 - val_acc: 0.9158\n",
      "- val_f1: 0.906103 - val_precision: 0.960199 - val_recall: 0.857778\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.7981 - acc: 0.7732 - val_loss: 0.2677 - val_acc: 0.9340\n",
      "- val_f1: 0.932205 - val_precision: 0.974086 - val_recall: 0.893778\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.7099 - acc: 0.7944 - val_loss: 0.2194 - val_acc: 0.9436\n",
      "- val_f1: 0.947187 - val_precision: 0.979810 - val_recall: 0.916667\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.6326 - acc: 0.8109 - val_loss: 0.1928 - val_acc: 0.9496\n",
      "- val_f1: 0.949594 - val_precision: 0.977642 - val_recall: 0.923111\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.5580 - acc: 0.8416 - val_loss: 0.3229 - val_acc: 0.9127\n",
      "- val_f1: 0.908733 - val_precision: 0.948960 - val_recall: 0.871778\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.5137 - acc: 0.8464 - val_loss: 0.1676 - val_acc: 0.9556\n",
      "- val_f1: 0.955618 - val_precision: 0.971527 - val_recall: 0.940222\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.4799 - acc: 0.8556 - val_loss: 0.1391 - val_acc: 0.9651\n",
      "- val_f1: 0.967983 - val_precision: 0.985941 - val_recall: 0.950667\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.4328 - acc: 0.8709 - val_loss: 0.1202 - val_acc: 0.9689\n",
      "- val_f1: 0.970883 - val_precision: 0.982480 - val_recall: 0.959556\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.4208 - acc: 0.8777 - val_loss: 0.1418 - val_acc: 0.9609\n",
      "- val_f1: 0.961599 - val_precision: 0.977722 - val_recall: 0.946000\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 0.3864 - acc: 0.8836 - val_loss: 0.1132 - val_acc: 0.9667\n",
      "- val_f1: 0.969357 - val_precision: 0.979360 - val_recall: 0.959556\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.3524 - acc: 0.8917 - val_loss: 0.1169 - val_acc: 0.9673\n",
      "- val_f1: 0.969853 - val_precision: 0.978295 - val_recall: 0.961556\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.3282 - acc: 0.9046 - val_loss: 0.1131 - val_acc: 0.9687\n",
      "- val_f1: 0.968233 - val_precision: 0.974775 - val_recall: 0.961778\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 0.3329 - acc: 0.9010 - val_loss: 0.1984 - val_acc: 0.9449\n",
      "- val_f1: 0.945062 - val_precision: 0.955692 - val_recall: 0.934667\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.3225 - acc: 0.9058 - val_loss: 0.1451 - val_acc: 0.9582\n",
      "- val_f1: 0.961651 - val_precision: 0.970575 - val_recall: 0.952889\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.3086 - acc: 0.9089 - val_loss: 0.1350 - val_acc: 0.9618\n",
      "- val_f1: 0.963204 - val_precision: 0.972587 - val_recall: 0.954000\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2928 - acc: 0.9161 - val_loss: 0.1534 - val_acc: 0.9589\n",
      "- val_f1: 0.958552 - val_precision: 0.963828 - val_recall: 0.953333\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.2705 - acc: 0.9184 - val_loss: 0.1283 - val_acc: 0.9647\n",
      "- val_f1: 0.963285 - val_precision: 0.967496 - val_recall: 0.959111\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.2678 - acc: 0.9204 - val_loss: 0.0511 - val_acc: 0.9847\n",
      "- val_f1: 0.985520 - val_precision: 0.987941 - val_recall: 0.983111\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.2630 - acc: 0.9220 - val_loss: 0.0754 - val_acc: 0.9789\n",
      "- val_f1: 0.980589 - val_precision: 0.984543 - val_recall: 0.976667\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2421 - acc: 0.9276 - val_loss: 0.0918 - val_acc: 0.9744\n",
      "- val_f1: 0.975735 - val_precision: 0.977475 - val_recall: 0.974000\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2452 - acc: 0.9288 - val_loss: 0.0877 - val_acc: 0.9758\n",
      "- val_f1: 0.975137 - val_precision: 0.978519 - val_recall: 0.971778\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.2494 - acc: 0.9282 - val_loss: 0.0918 - val_acc: 0.9738\n",
      "- val_f1: 0.976039 - val_precision: 0.978985 - val_recall: 0.973111\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 99s 394ms/step - loss: 0.2364 - acc: 0.9313 - val_loss: 0.0511 - val_acc: 0.9862\n",
      "- val_f1: 0.985749 - val_precision: 0.987729 - val_recall: 0.983778\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.2241 - acc: 0.9330 - val_loss: 0.0784 - val_acc: 0.9776\n",
      "- val_f1: 0.977837 - val_precision: 0.980129 - val_recall: 0.975556\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.2202 - acc: 0.9364 - val_loss: 0.0550 - val_acc: 0.9871\n",
      "- val_f1: 0.986857 - val_precision: 0.989281 - val_recall: 0.984444\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 99s 396ms/step - loss: 0.2041 - acc: 0.9407 - val_loss: 0.1193 - val_acc: 0.9704\n",
      "- val_f1: 0.969981 - val_precision: 0.974221 - val_recall: 0.965778\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.2145 - acc: 0.9366 - val_loss: 0.1232 - val_acc: 0.9718\n",
      "- val_f1: 0.973286 - val_precision: 0.975022 - val_recall: 0.971556\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 98s 393ms/step - loss: 0.2179 - acc: 0.9430 - val_loss: 0.2401 - val_acc: 0.9367\n",
      "- val_f1: 0.939261 - val_precision: 0.949172 - val_recall: 0.929556\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.2205 - acc: 0.9396 - val_loss: 0.1103 - val_acc: 0.9718\n",
      "- val_f1: 0.971811 - val_precision: 0.974525 - val_recall: 0.969111\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 100s 402ms/step - loss: 0.2115 - acc: 0.9413 - val_loss: 0.0736 - val_acc: 0.9816\n",
      "- val_f1: 0.982390 - val_precision: 0.985465 - val_recall: 0.979333\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 101s 403ms/step - loss: 0.2101 - acc: 0.9456 - val_loss: 0.2560 - val_acc: 0.9460\n",
      "- val_f1: 0.948543 - val_precision: 0.952904 - val_recall: 0.944222\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2122 - acc: 0.9443 - val_loss: 0.0809 - val_acc: 0.9758\n",
      "- val_f1: 0.977486 - val_precision: 0.980546 - val_recall: 0.974444\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2070 - acc: 0.9432 - val_loss: 0.1124 - val_acc: 0.9702\n",
      "- val_f1: 0.971906 - val_precision: 0.975168 - val_recall: 0.968667\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2066 - acc: 0.9431 - val_loss: 0.1448 - val_acc: 0.9653\n",
      "- val_f1: 0.965479 - val_precision: 0.967634 - val_recall: 0.963333\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2181 - acc: 0.9432 - val_loss: 0.1602 - val_acc: 0.9593\n",
      "- val_f1: 0.959446 - val_precision: 0.964727 - val_recall: 0.954222\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.1914 - acc: 0.9459 - val_loss: 0.0701 - val_acc: 0.9869\n",
      "- val_f1: 0.984537 - val_precision: 0.985743 - val_recall: 0.983333\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.1861 - acc: 0.9477 - val_loss: 0.2712 - val_acc: 0.9462\n",
      "- val_f1: 0.945666 - val_precision: 0.949585 - val_recall: 0.941778\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2104 - acc: 0.9441 - val_loss: 0.1480 - val_acc: 0.9704\n",
      "- val_f1: 0.968058 - val_precision: 0.969677 - val_recall: 0.966444\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.1902 - acc: 0.9463 - val_loss: 0.1113 - val_acc: 0.9704\n",
      "- val_f1: 0.970926 - val_precision: 0.973420 - val_recall: 0.968444\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.1926 - acc: 0.9464 - val_loss: 0.0904 - val_acc: 0.9776\n",
      "- val_f1: 0.976972 - val_precision: 0.978169 - val_recall: 0.975778\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.1956 - acc: 0.9490 - val_loss: 0.2469 - val_acc: 0.9487\n",
      "- val_f1: 0.947779 - val_precision: 0.949788 - val_recall: 0.945778\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.1900 - acc: 0.9504 - val_loss: 0.0683 - val_acc: 0.9827\n",
      "- val_f1: 0.983975 - val_precision: 0.985510 - val_recall: 0.982444\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.1938 - acc: 0.9474 - val_loss: 0.0698 - val_acc: 0.9818\n",
      "- val_f1: 0.982972 - val_precision: 0.984615 - val_recall: 0.981333\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2013 - acc: 0.9452 - val_loss: 0.1032 - val_acc: 0.9767\n",
      "- val_f1: 0.978312 - val_precision: 0.979292 - val_recall: 0.977333\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.1845 - acc: 0.9500 - val_loss: 0.1168 - val_acc: 0.9729\n",
      "- val_f1: 0.974079 - val_precision: 0.975273 - val_recall: 0.972889\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 101s 402ms/step - loss: 0.2232 - acc: 0.9440 - val_loss: 0.0625 - val_acc: 0.9847\n",
      "- val_f1: 0.985752 - val_precision: 0.987511 - val_recall: 0.984000\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2255 - acc: 0.9424 - val_loss: 0.1132 - val_acc: 0.9736\n",
      "- val_f1: 0.977194 - val_precision: 0.978392 - val_recall: 0.976000\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.1976 - acc: 0.9479 - val_loss: 0.0614 - val_acc: 0.9847\n",
      "- val_f1: 0.986210 - val_precision: 0.987088 - val_recall: 0.985333\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.1918 - acc: 0.9502 - val_loss: 0.1421 - val_acc: 0.9653\n",
      "- val_f1: 0.966109 - val_precision: 0.969351 - val_recall: 0.962889\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2060 - acc: 0.9467 - val_loss: 0.1151 - val_acc: 0.9720\n",
      "- val_f1: 0.972497 - val_precision: 0.974559 - val_recall: 0.970444\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2060 - acc: 0.9482 - val_loss: 0.1026 - val_acc: 0.9736\n",
      "- val_f1: 0.973388 - val_precision: 0.975452 - val_recall: 0.971333\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2093 - acc: 0.9477 - val_loss: 0.0861 - val_acc: 0.9807\n",
      "- val_f1: 0.981753 - val_precision: 0.983066 - val_recall: 0.980444\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2206 - acc: 0.9481 - val_loss: 0.0958 - val_acc: 0.9787\n",
      "- val_f1: 0.981082 - val_precision: 0.982613 - val_recall: 0.979556\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 101s 402ms/step - loss: 0.2081 - acc: 0.9501 - val_loss: 0.1961 - val_acc: 0.9556\n",
      "- val_f1: 0.954312 - val_precision: 0.957085 - val_recall: 0.951556\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2030 - acc: 0.9496 - val_loss: 0.2003 - val_acc: 0.9676\n",
      "- val_f1: 0.970418 - val_precision: 0.971282 - val_recall: 0.969556\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2267 - acc: 0.9467 - val_loss: 0.0349 - val_acc: 0.9907\n",
      "- val_f1: 0.991442 - val_precision: 0.991772 - val_recall: 0.991111\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2064 - acc: 0.9497 - val_loss: 0.1922 - val_acc: 0.9647\n",
      "- val_f1: 0.966347 - val_precision: 0.969155 - val_recall: 0.963556\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.2252 - acc: 0.9458 - val_loss: 0.1479 - val_acc: 0.9691\n",
      "- val_f1: 0.970257 - val_precision: 0.972750 - val_recall: 0.967778\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.2082 - acc: 0.9464 - val_loss: 0.1533 - val_acc: 0.9642\n",
      "- val_f1: 0.961379 - val_precision: 0.968644 - val_recall: 0.954222\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2476 - acc: 0.9458 - val_loss: 0.1203 - val_acc: 0.9756\n",
      "- val_f1: 0.975072 - val_precision: 0.976594 - val_recall: 0.973556\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 100s 402ms/step - loss: 0.2378 - acc: 0.9489 - val_loss: 0.1934 - val_acc: 0.9649\n",
      "- val_f1: 0.965879 - val_precision: 0.969338 - val_recall: 0.962444\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.2293 - acc: 0.9482 - val_loss: 0.2622 - val_acc: 0.9562\n",
      "- val_f1: 0.958361 - val_precision: 0.960286 - val_recall: 0.956444\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2298 - acc: 0.9499 - val_loss: 0.1301 - val_acc: 0.9787\n",
      "- val_f1: 0.977872 - val_precision: 0.978633 - val_recall: 0.977111\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2432 - acc: 0.9446 - val_loss: 0.2514 - val_acc: 0.9531\n",
      "- val_f1: 0.953209 - val_precision: 0.955764 - val_recall: 0.950667\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2340 - acc: 0.9476 - val_loss: 0.2178 - val_acc: 0.9698\n",
      "- val_f1: 0.967397 - val_precision: 0.968799 - val_recall: 0.966000\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2166 - acc: 0.9508 - val_loss: 0.2091 - val_acc: 0.9600\n",
      "- val_f1: 0.960294 - val_precision: 0.963950 - val_recall: 0.956667\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.2392 - acc: 0.9471 - val_loss: 0.2017 - val_acc: 0.9689\n",
      "- val_f1: 0.968726 - val_precision: 0.970346 - val_recall: 0.967111\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 101s 405ms/step - loss: 0.2420 - acc: 0.9500 - val_loss: 0.2863 - val_acc: 0.9429\n",
      "- val_f1: 0.944023 - val_precision: 0.947404 - val_recall: 0.940667\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 100s 402ms/step - loss: 0.2337 - acc: 0.9494 - val_loss: 0.1429 - val_acc: 0.9736\n",
      "- val_f1: 0.974422 - val_precision: 0.975289 - val_recall: 0.973556\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 101s 402ms/step - loss: 0.2143 - acc: 0.9517 - val_loss: 0.1646 - val_acc: 0.9673\n",
      "- val_f1: 0.968941 - val_precision: 0.970778 - val_recall: 0.967111\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2331 - acc: 0.9491 - val_loss: 0.1700 - val_acc: 0.9676\n",
      "- val_f1: 0.968632 - val_precision: 0.969710 - val_recall: 0.967556\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2565 - acc: 0.9474 - val_loss: 0.0624 - val_acc: 0.9849\n",
      "- val_f1: 0.985756 - val_precision: 0.987294 - val_recall: 0.984222\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2030 - acc: 0.9560 - val_loss: 0.0961 - val_acc: 0.9800\n",
      "- val_f1: 0.978847 - val_precision: 0.980812 - val_recall: 0.976889\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 101s 402ms/step - loss: 0.2357 - acc: 0.9472 - val_loss: 0.0467 - val_acc: 0.9887\n",
      "- val_f1: 0.987209 - val_precision: 0.988199 - val_recall: 0.986222\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2335 - acc: 0.9501 - val_loss: 0.3121 - val_acc: 0.9489\n",
      "- val_f1: 0.951841 - val_precision: 0.952794 - val_recall: 0.950889\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2181 - acc: 0.9529 - val_loss: 0.1418 - val_acc: 0.9747\n",
      "- val_f1: 0.976201 - val_precision: 0.977070 - val_recall: 0.975333\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2227 - acc: 0.9501 - val_loss: 0.2500 - val_acc: 0.9622\n",
      "- val_f1: 0.963622 - val_precision: 0.964803 - val_recall: 0.962444\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2156 - acc: 0.9550 - val_loss: 0.3199 - val_acc: 0.9536\n",
      "- val_f1: 0.951534 - val_precision: 0.951957 - val_recall: 0.951111\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 100s 402ms/step - loss: 0.2328 - acc: 0.9516 - val_loss: 0.3617 - val_acc: 0.9458\n",
      "- val_f1: 0.949077 - val_precision: 0.949711 - val_recall: 0.948444\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 100s 402ms/step - loss: 0.2432 - acc: 0.9489 - val_loss: 1.0317 - val_acc: 0.8724\n",
      "- val_f1: 0.879848 - val_precision: 0.882603 - val_recall: 0.877111\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 101s 402ms/step - loss: 0.2540 - acc: 0.9486 - val_loss: 0.3009 - val_acc: 0.9509\n",
      "- val_f1: 0.951119 - val_precision: 0.953135 - val_recall: 0.949111\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 101s 404ms/step - loss: 0.2747 - acc: 0.9480 - val_loss: 0.4707 - val_acc: 0.9309\n",
      "- val_f1: 0.935786 - val_precision: 0.938926 - val_recall: 0.932667\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2678 - acc: 0.9497 - val_loss: 0.0764 - val_acc: 0.9856\n",
      "- val_f1: 0.986548 - val_precision: 0.987097 - val_recall: 0.986000\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 99s 398ms/step - loss: 0.2570 - acc: 0.9466 - val_loss: 0.5343 - val_acc: 0.9251\n",
      "- val_f1: 0.921811 - val_precision: 0.922734 - val_recall: 0.920889\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2378 - acc: 0.9532 - val_loss: 0.1107 - val_acc: 0.9784\n",
      "- val_f1: 0.977976 - val_precision: 0.979065 - val_recall: 0.976889\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.2602 - acc: 0.9509 - val_loss: 0.1459 - val_acc: 0.9722\n",
      "- val_f1: 0.972504 - val_precision: 0.974348 - val_recall: 0.970667\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2617 - acc: 0.9504 - val_loss: 0.1040 - val_acc: 0.9758\n",
      "- val_f1: 0.975271 - val_precision: 0.982195 - val_recall: 0.968444\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 100s 400ms/step - loss: 0.2586 - acc: 0.9458 - val_loss: 0.1333 - val_acc: 0.9667\n",
      "- val_f1: 0.964795 - val_precision: 0.967382 - val_recall: 0.962222\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.2557 - acc: 0.9488 - val_loss: 0.3345 - val_acc: 0.9424\n",
      "- val_f1: 0.940783 - val_precision: 0.944258 - val_recall: 0.937333\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 100s 398ms/step - loss: 0.2712 - acc: 0.9484 - val_loss: 0.6046 - val_acc: 0.9320\n",
      "- val_f1: 0.930471 - val_precision: 0.931611 - val_recall: 0.929333\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 100s 399ms/step - loss: 0.2514 - acc: 0.9511 - val_loss: 0.2408 - val_acc: 0.9616\n",
      "- val_f1: 0.961568 - val_precision: 0.964038 - val_recall: 0.959111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fc4562b70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Front images training:\")\n",
    "\n",
    "model_front.fit_generator(\n",
    "    front_train_gen,\n",
    "    steps_per_epoch = 250,\n",
    "    epochs = 100,\n",
    "    validation_data = front_validation_gen,\n",
    "    validation_steps = 125,\n",
    "    callbacks = [MetricsFront()])\n",
    "\n",
    "print(\"\\nBack images training:\")\n",
    "\n",
    "model_back.fit_generator(\n",
    "    back_train_gen,\n",
    "    steps_per_epoch = 250,\n",
    "    epochs = 100,\n",
    "    validation_data = back_validation_gen,\n",
    "    validation_steps = 125,\n",
    "    callbacks = [MetricsBack()])\n",
    "\n",
    "print(\"\\nHighlighted images training:\")\n",
    "\n",
    "model_high.fit_generator(\n",
    "    high_train_gen,\n",
    "    steps_per_epoch = 250,\n",
    "    epochs = 100,\n",
    "    validation_data = high_validation_gen,\n",
    "    validation_steps = 125,\n",
    "    callbacks = [MetricsHigh()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. 尋找最佳模型並繪製 F1 Score 趨勢圖 \n",
    "根據實驗結果的圖形與數據顯示，排藥**正面影像**在 42 Epochs 時即達到最佳識別效果；排藥**背面影像**則在 67 Epochs 達到最佳效果。\n",
    "\n",
    "很明顯的，排藥背面由於文字或圖形等易於識別的特徵，使得識別效果優於排藥正面影像。\n",
    "\n",
    "而在**雙面拼整影像**的表現上，模型在 20 個 Epoch 左右即達到 95% 以上的識別效果，在 86 Epochs 時更達到了 98.71%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Front]\t\t32 epochs reach the best f1-score value 68.39%\n",
      "[Back]\t\t40 epochs reach the best f1-score value 74.66%\n",
      "[Highlighted]\t65 epochs reach the best f1-score value 99.14%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_front_epochs = front_f1s.index(max(front_f1s)) + 1\n",
    "best_back_epochs = back_f1s.index(max(back_f1s)) + 1\n",
    "best_high_epochs = high_f1s.index(max(high_f1s)) + 1\n",
    "\n",
    "print(\"[Front]\\t\\t%d epochs reach the best f1-score value %.2f%%\"%(best_front_epochs, max(front_f1s) * 100))\n",
    "print(\"[Back]\\t\\t%d epochs reach the best f1-score value %.2f%%\"%(best_back_epochs, max(back_f1s) * 100))\n",
    "print(\"[Highlighted]\\t%d epochs reach the best f1-score value %.2f%%\"%(best_high_epochs, max(high_f1s) * 100))\n",
    "\n",
    "plt.plot(range(100), front_f1s, 'b-o', label = 'Front images')\n",
    "plt.plot(range(100), back_f1s, 'g-o', label = 'Back images')\n",
    "plt.plot(range(100), high_f1s, 'r-o', label = 'Highlighted images')\n",
    "plt.legend()\n",
    "plt.title('Validation F1-score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(0, 100, step = 10), range(1, 101, 10))\n",
    "plt.ylabel('F1-score')\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. 使用完整資料重新訓練模型\n",
    "根據前述的實驗結果，我將以識別效果最佳的 Epoch 次數作為訓練的 Early Stopping 時機，**採用完整的訓練資料集**（不切分驗證集），重新對相同的網路結構進行訓練。\n",
    "\n",
    "資料總計：訓練樣本 13,500 張影像、測試樣本 4,500 張影像，共 250 類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of front images:\n",
      "[Training set]\n",
      "Found 13500 images belonging to 250 classes.\n",
      "[Testing set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Status of back images:\n",
      "[Training set]\n",
      "Found 13500 images belonging to 250 classes.\n",
      "[Testing set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Status of highlighted images:\n",
      "[Training set]\n",
      "Found 13500 images belonging to 250 classes.\n",
      "[Testing set]\n",
      "Found 4500 images belonging to 250 classes.\n",
      "\n",
      "Front images training:\n",
      "Epoch 1/32\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 5.3787 - acc: 0.0080\n",
      "Epoch 2/32\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 4.9128 - acc: 0.0280\n",
      "Epoch 3/32\n",
      "375/375 [==============================] - 133s 354ms/step - loss: 4.2981 - acc: 0.0744\n",
      "Epoch 4/32\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 3.7502 - acc: 0.1377\n",
      "Epoch 5/32\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 3.2850 - acc: 0.1989\n",
      "Epoch 6/32\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 2.8898 - acc: 0.2639\n",
      "Epoch 7/32\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 2.5798 - acc: 0.3204\n",
      "Epoch 8/32\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 2.3365 - acc: 0.3719\n",
      "Epoch 9/32\n",
      "375/375 [==============================] - 132s 353ms/step - loss: 2.1311 - acc: 0.4164\n",
      "Epoch 10/32\n",
      "375/375 [==============================] - 133s 353ms/step - loss: 1.9576 - acc: 0.4539\n",
      "Epoch 11/32\n",
      "375/375 [==============================] - 132s 353ms/step - loss: 1.8452 - acc: 0.4769\n",
      "Epoch 12/32\n",
      "375/375 [==============================] - 133s 354ms/step - loss: 1.7264 - acc: 0.5074\n",
      "Epoch 13/32\n",
      "375/375 [==============================] - 132s 353ms/step - loss: 1.6235 - acc: 0.5305\n",
      "Epoch 14/32\n",
      "375/375 [==============================] - 133s 354ms/step - loss: 1.5469 - acc: 0.5510\n",
      "Epoch 15/32\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 1.4898 - acc: 0.5638\n",
      "Epoch 16/32\n",
      "375/375 [==============================] - 135s 359ms/step - loss: 1.4273 - acc: 0.5781\n",
      "Epoch 17/32\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 1.3741 - acc: 0.5948\n",
      "Epoch 18/32\n",
      "375/375 [==============================] - 135s 359ms/step - loss: 1.3237 - acc: 0.6079\n",
      "Epoch 19/32\n",
      "375/375 [==============================] - 135s 360ms/step - loss: 1.2935 - acc: 0.6207\n",
      "Epoch 20/32\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 1.2380 - acc: 0.6284\n",
      "Epoch 21/32\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.2266 - acc: 0.6404\n",
      "Epoch 22/32\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1887 - acc: 0.6499\n",
      "Epoch 23/32\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1790 - acc: 0.6530\n",
      "Epoch 24/32\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 1.1432 - acc: 0.6605\n",
      "Epoch 25/32\n",
      "375/375 [==============================] - 134s 356ms/step - loss: 1.1357 - acc: 0.6612\n",
      "Epoch 26/32\n",
      "375/375 [==============================] - 133s 355ms/step - loss: 1.1432 - acc: 0.6650\n",
      "Epoch 27/32\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.1143 - acc: 0.6696\n",
      "Epoch 28/32\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 1.1333 - acc: 0.6683\n",
      "Epoch 29/32\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.0950 - acc: 0.6804\n",
      "Epoch 30/32\n",
      "375/375 [==============================] - 133s 356ms/step - loss: 1.0996 - acc: 0.6799\n",
      "Epoch 31/32\n",
      "375/375 [==============================] - 134s 357ms/step - loss: 1.1110 - acc: 0.6812\n",
      "Epoch 32/32\n",
      "375/375 [==============================] - 134s 358ms/step - loss: 1.0847 - acc: 0.6821\n",
      "\n",
      "Back images training:\n",
      "Epoch 1/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 5.4619 - acc: 0.0073\n",
      "Epoch 2/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 5.0335 - acc: 0.0227\n",
      "Epoch 3/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 4.3410 - acc: 0.0694\n",
      "Epoch 4/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 3.7259 - acc: 0.1315\n",
      "Epoch 5/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 3.2384 - acc: 0.1939\n",
      "Epoch 6/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 2.8821 - acc: 0.2550\n",
      "Epoch 7/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 2.5402 - acc: 0.3172\n",
      "Epoch 8/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 2.3214 - acc: 0.3644\n",
      "Epoch 9/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 2.1222 - acc: 0.4156\n",
      "Epoch 10/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.9525 - acc: 0.4564\n",
      "Epoch 11/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.7939 - acc: 0.4885\n",
      "Epoch 12/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.6847 - acc: 0.5130\n",
      "Epoch 13/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.5996 - acc: 0.5410\n",
      "Epoch 14/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.5082 - acc: 0.5590\n",
      "Epoch 15/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.4428 - acc: 0.5799\n",
      "Epoch 16/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.3611 - acc: 0.6027\n",
      "Epoch 17/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.3255 - acc: 0.6164\n",
      "Epoch 18/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.2589 - acc: 0.6276\n",
      "Epoch 19/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.2165 - acc: 0.6376\n",
      "Epoch 20/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.1779 - acc: 0.6474\n",
      "Epoch 21/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.1370 - acc: 0.6654\n",
      "Epoch 22/40\n",
      "375/375 [==============================] - 140s 374ms/step - loss: 1.1180 - acc: 0.6673\n",
      "Epoch 23/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.1045 - acc: 0.6767\n",
      "Epoch 24/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.0417 - acc: 0.6877\n",
      "Epoch 25/40\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 1.0435 - acc: 0.6926\n",
      "Epoch 26/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 1.0559 - acc: 0.6921\n",
      "Epoch 27/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 1.0426 - acc: 0.6974\n",
      "Epoch 28/40\n",
      "375/375 [==============================] - 142s 377ms/step - loss: 1.0003 - acc: 0.7087\n",
      "Epoch 29/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 0.9955 - acc: 0.7101\n",
      "Epoch 30/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 0.9847 - acc: 0.7090\n",
      "Epoch 31/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 0.9719 - acc: 0.7202\n",
      "Epoch 32/40\n",
      "375/375 [==============================] - 141s 377ms/step - loss: 0.9701 - acc: 0.7156\n",
      "Epoch 33/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 0.9834 - acc: 0.7173\n",
      "Epoch 34/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 0.9818 - acc: 0.7233\n",
      "Epoch 35/40\n",
      "375/375 [==============================] - 141s 375ms/step - loss: 0.9819 - acc: 0.7235\n",
      "Epoch 36/40\n",
      "375/375 [==============================] - 140s 374ms/step - loss: 0.9826 - acc: 0.7233\n",
      "Epoch 37/40\n",
      "375/375 [==============================] - 140s 374ms/step - loss: 1.0053 - acc: 0.7220\n",
      "Epoch 38/40\n",
      "375/375 [==============================] - 140s 374ms/step - loss: 1.0025 - acc: 0.7184\n",
      "Epoch 39/40\n",
      "375/375 [==============================] - 140s 374ms/step - loss: 1.0101 - acc: 0.7193\n",
      "Epoch 40/40\n",
      "375/375 [==============================] - 141s 376ms/step - loss: 0.9989 - acc: 0.7268\n",
      "\n",
      "Highlighted images training:\n",
      "Epoch 1/65\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 5.2180 - acc: 0.0196\n",
      "Epoch 2/65\n",
      "375/375 [==============================] - 110s 293ms/step - loss: 3.8936 - acc: 0.1483\n",
      "Epoch 3/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 2.7091 - acc: 0.3384\n",
      "Epoch 4/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 1.9724 - acc: 0.4867\n",
      "Epoch 5/65\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 1.4982 - acc: 0.5941\n",
      "Epoch 6/65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 109s 291ms/step - loss: 1.1748 - acc: 0.6784\n",
      "Epoch 7/65\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.9672 - acc: 0.7270\n",
      "Epoch 8/65\n",
      "375/375 [==============================] - 110s 293ms/step - loss: 0.8095 - acc: 0.7696\n",
      "Epoch 9/65\n",
      "375/375 [==============================] - 111s 295ms/step - loss: 0.6751 - acc: 0.8032\n",
      "Epoch 10/65\n",
      "375/375 [==============================] - 110s 295ms/step - loss: 0.5965 - acc: 0.8235\n",
      "Epoch 11/65\n",
      "375/375 [==============================] - 111s 295ms/step - loss: 0.5523 - acc: 0.8407\n",
      "Epoch 12/65\n",
      "375/375 [==============================] - 110s 293ms/step - loss: 0.4882 - acc: 0.8565\n",
      "Epoch 13/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.4277 - acc: 0.8737\n",
      "Epoch 14/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.4072 - acc: 0.8816\n",
      "Epoch 15/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.3710 - acc: 0.8896\n",
      "Epoch 16/65\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.3498 - acc: 0.8967\n",
      "Epoch 17/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.3274 - acc: 0.9033\n",
      "Epoch 18/65\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.3085 - acc: 0.9095\n",
      "Epoch 19/65\n",
      "375/375 [==============================] - 112s 298ms/step - loss: 0.2867 - acc: 0.9179\n",
      "Epoch 20/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2678 - acc: 0.9186\n",
      "Epoch 21/65\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2610 - acc: 0.9261\n",
      "Epoch 22/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2533 - acc: 0.9260\n",
      "Epoch 23/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2437 - acc: 0.9256\n",
      "Epoch 24/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2534 - acc: 0.9269\n",
      "Epoch 25/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2415 - acc: 0.9305\n",
      "Epoch 26/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2275 - acc: 0.9349\n",
      "Epoch 27/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2361 - acc: 0.9333\n",
      "Epoch 28/65\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2293 - acc: 0.9365\n",
      "Epoch 29/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2214 - acc: 0.9360\n",
      "Epoch 30/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2292 - acc: 0.9385\n",
      "Epoch 31/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2304 - acc: 0.9376\n",
      "Epoch 32/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2319 - acc: 0.9370\n",
      "Epoch 33/65\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2289 - acc: 0.9375\n",
      "Epoch 34/65\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2171 - acc: 0.9419\n",
      "Epoch 35/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2192 - acc: 0.9415\n",
      "Epoch 36/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2265 - acc: 0.9405\n",
      "Epoch 37/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2241 - acc: 0.9399\n",
      "Epoch 38/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2301 - acc: 0.9429\n",
      "Epoch 39/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2240 - acc: 0.9418\n",
      "Epoch 40/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2325 - acc: 0.9408\n",
      "Epoch 41/65\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2406 - acc: 0.9419\n",
      "Epoch 42/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2257 - acc: 0.9438\n",
      "Epoch 43/65\n",
      "375/375 [==============================] - 111s 296ms/step - loss: 0.2279 - acc: 0.9419\n",
      "Epoch 44/65\n",
      "375/375 [==============================] - 111s 295ms/step - loss: 0.2260 - acc: 0.9444\n",
      "Epoch 45/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2467 - acc: 0.9412\n",
      "Epoch 46/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2513 - acc: 0.9412\n",
      "Epoch 47/65\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2386 - acc: 0.9430\n",
      "Epoch 48/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2182 - acc: 0.9477\n",
      "Epoch 49/65\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2363 - acc: 0.9459\n",
      "Epoch 50/65\n",
      "375/375 [==============================] - 109s 290ms/step - loss: 0.2667 - acc: 0.9418\n",
      "Epoch 51/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2602 - acc: 0.9420\n",
      "Epoch 52/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2533 - acc: 0.9413\n",
      "Epoch 53/65\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2339 - acc: 0.9478\n",
      "Epoch 54/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2461 - acc: 0.9454\n",
      "Epoch 55/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2448 - acc: 0.9458\n",
      "Epoch 56/65\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2600 - acc: 0.9453\n",
      "Epoch 57/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2489 - acc: 0.9452\n",
      "Epoch 58/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2818 - acc: 0.9420\n",
      "Epoch 59/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2421 - acc: 0.9462\n",
      "Epoch 60/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2600 - acc: 0.9441\n",
      "Epoch 61/65\n",
      "375/375 [==============================] - 110s 293ms/step - loss: 0.2525 - acc: 0.9462\n",
      "Epoch 62/65\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 0.2442 - acc: 0.9492\n",
      "Epoch 63/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2570 - acc: 0.9473\n",
      "Epoch 64/65\n",
      "375/375 [==============================] - 109s 291ms/step - loss: 0.2700 - acc: 0.9459\n",
      "Epoch 65/65\n",
      "375/375 [==============================] - 109s 292ms/step - loss: 0.2875 - acc: 0.9448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0e84454ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新建立卷積神經網路\n",
    "model_front = models.Sequential()\n",
    "model_front.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_front.add(layers.MaxPooling2D((2, 2)))\n",
    "model_front.add(Dropout(0.25))\n",
    "model_front.add(layers.Flatten())\n",
    "model_front.add(layers.Dense(128, activation = 'relu'))\n",
    "model_front.add(Dropout(0.5))\n",
    "model_front.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "model_back = models.Sequential()\n",
    "model_back.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_back.add(layers.MaxPooling2D((2, 2)))\n",
    "model_back.add(Dropout(0.25))\n",
    "model_back.add(layers.Flatten())\n",
    "model_back.add(layers.Dense(128, activation = 'relu'))\n",
    "model_back.add(Dropout(0.5))\n",
    "model_back.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "model_high = models.Sequential()\n",
    "model_high.add(layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model_high.add(layers.MaxPooling2D((2, 2)))\n",
    "model_high.add(Dropout(0.25))\n",
    "model_high.add(layers.Flatten())\n",
    "model_high.add(layers.Dense(128, activation = 'relu'))\n",
    "model_high.add(Dropout(0.5))\n",
    "model_high.add(layers.Dense(250, activation = 'softmax'))\n",
    "\n",
    "# 重新編譯模型\n",
    "model_front.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "model_back.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "model_high.compile(\n",
    "    optimizer = 'rmsprop',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "# 重新製作數據 Generator\n",
    "data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"Status of front images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "front_train_gen = data_generator.flow_from_directory(\n",
    "        front_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"[Testing set]\")\n",
    "\n",
    "front_test_gen = data_generator.flow_from_directory(\n",
    "        front_test_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"\\nStatus of back images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "back_train_gen = data_generator.flow_from_directory(\n",
    "        back_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"[Testing set]\")\n",
    "\n",
    "back_test_gen = data_generator.flow_from_directory(\n",
    "        back_test_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"\\nStatus of highlighted images:\")\n",
    "\n",
    "print(\"[Training set]\")\n",
    "\n",
    "high_train_gen = data_generator.flow_from_directory(\n",
    "        high_train_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "print(\"[Testing set]\")\n",
    "\n",
    "high_test_gen = data_generator.flow_from_directory(\n",
    "        high_test_dir,\n",
    "        target_size = (image_size, image_size),\n",
    "        shuffle = True,\n",
    "        seed = 13,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = 36)\n",
    "\n",
    "# 設定 Callback 函式並在訓練完成時，計算測試資料集的 F1-score\n",
    "test_f1s = []\n",
    "\n",
    "class MetricsFront(Callback):\n",
    "\n",
    "    def __init__(self, test_data = front_test_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        batches = len(self.test_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        test_pred = np.zeros((total, 250))\n",
    "        test_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xTest, yTest = next(self.test_data)\n",
    "            test_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xTest)).round()\n",
    "            test_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yTest\n",
    "            \n",
    "        test_pred = np.squeeze(test_pred)\n",
    "        _test_f1 = f1_score(test_true, test_pred, average = 'micro')\n",
    "        test_f1s.append(_test_f1)\n",
    "        \n",
    "        return\n",
    "    \n",
    "class MetricsBack(Callback):\n",
    "\n",
    "    def __init__(self, test_data = back_test_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        batches = len(self.test_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        test_pred = np.zeros((total, 250))\n",
    "        test_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xTest, yTest = next(self.test_data)\n",
    "            test_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xTest)).round()\n",
    "            test_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yTest\n",
    "            \n",
    "        test_pred = np.squeeze(test_pred)\n",
    "        _test_f1 = f1_score(test_true, test_pred, average = 'micro')\n",
    "        test_f1s.append(_test_f1)\n",
    "        \n",
    "        return\n",
    "    \n",
    "class MetricsHigh(Callback):\n",
    "\n",
    "    def __init__(self, test_data = high_test_gen, batch_size = 36):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_train_end(self, epoch, logs={}):\n",
    "        batches = len(self.test_data)\n",
    "        total = batches * self.batch_size\n",
    "        \n",
    "        test_pred = np.zeros((total, 250))\n",
    "        test_true = np.zeros((total, 250))\n",
    "        \n",
    "        for batch in range(batches):\n",
    "            xTest, yTest = next(self.test_data)\n",
    "            test_pred[batch * self.batch_size : (batch + 1) * self.batch_size] = np.asarray(self.model.predict(xTest)).round()\n",
    "            test_true[batch * self.batch_size : (batch + 1) * self.batch_size] = yTest\n",
    "            \n",
    "        test_pred = np.squeeze(test_pred)\n",
    "        _test_f1 = f1_score(test_true, test_pred, average = 'micro')\n",
    "        test_f1s.append(_test_f1)\n",
    "\n",
    "        return\n",
    "    \n",
    "# 重新訓練深度學習網路\n",
    "print(\"\\nFront images training:\")\n",
    "\n",
    "model_front.fit_generator(\n",
    "    front_train_gen,\n",
    "    steps_per_epoch = 375,\n",
    "    epochs = best_front_epochs,\n",
    "    callbacks = [MetricsFront()])\n",
    "\n",
    "print(\"\\nBack images training:\")\n",
    "\n",
    "model_back.fit_generator(\n",
    "    back_train_gen,\n",
    "    steps_per_epoch = 375,\n",
    "    epochs = best_back_epochs,\n",
    "    callbacks = [MetricsBack()])\n",
    "\n",
    "print(\"\\nHighlighted images training:\")\n",
    "\n",
    "model_high.fit_generator(\n",
    "    high_train_gen,\n",
    "    steps_per_epoch = 375,\n",
    "    epochs = best_high_epochs,\n",
    "    callbacks = [MetricsHigh()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實驗總結\n",
    "排藥正面與背面影像，在測試資料集上僅能達到 67.43% 與 50.73% 的識別效果；而雙面拼整影像則達到了 98.53%。\n",
    "\n",
    "這些現象都可證明「誘導式深度學習」技術，對於樣本稀少、外觀相似且種類繁多的排藥辨識問題，不但能提供實際且有效的解決方案，還能套用於任何一種深度學習網路結構，使該網路的特徵學習與識別能力大幅提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TESTING RESULT ---\n",
      "[Front]\t\tThe f1-score of front testing set: 58.52%\n",
      "[Back]\t\tThe f1-score of back testing set: 75.79%\n",
      "[Highlighted]\tThe f1-score of highlighted testing set: 97.94%\n"
     ]
    }
   ],
   "source": [
    "print(\"--- TESTING RESULT ---\")\n",
    "print(\"[Front]\\t\\tThe f1-score of front testing set: %.2f%%\"%(test_f1s[0] * 100))\n",
    "print(\"[Back]\\t\\tThe f1-score of back testing set: %.2f%%\"%(test_f1s[1] * 100))\n",
    "print(\"[Highlighted]\\tThe f1-score of highlighted testing set: %.2f%%\"%(test_f1s[2] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
